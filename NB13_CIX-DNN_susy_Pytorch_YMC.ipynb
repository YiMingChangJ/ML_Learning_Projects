{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 13: Using Deep Learning to Study SUSY with Pytorch\n",
    "Yi Ming Chang <br>\n",
    "B00751897<br>\n",
    "Mar. 7th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "The goal of this notebook is to introduce the powerful PyTorch framework for building neural networks and use it to analyze the SUSY dataset. After this notebook, the reader should understand the mechanics of PyTorch and how to construct DNNs using this package. In addition, the reader is encouraged to explore the GPU backend available in Pytorch on this dataset.\n",
    "\n",
    "## Overview\n",
    "In this notebook, we use Deep Neural Networks to classify the supersymmetry dataset, first introduced by Baldi et al. in [Nature Communication (2015)](https://www.nature.com/articles/ncomms5308). The SUSY data set consists of 5,000,000 Monte-Carlo samples of supersymmetric and non-supersymmetric collisions with $18$ features. The signal process is the production of electrically-charged supersymmetric particles which decay to $W$ bosons and an electrically-neutral supersymmetric particle that is invisible to the detector.\n",
    "\n",
    "The first $8$ features are \"raw\" kinematic features that can be directly measured from collisions. The final $10$ features are \"hand constructed\" features that have been chosen using physical knowledge and are known to be important in distinguishing supersymmetric and non-supersymmetric collision events. More specifically, they are given by the column names below.\n",
    "\n",
    "In this notebook, we study this dataset using Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without CUDA commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os,sys\n",
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "import torch # pytorch package, allows using GPUs\n",
    "# fix seed\n",
    "seed=17\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we define a helper function `load_data()` that accepts as a required argument the set of parameters `args`, and returns two generators: `test_loader` and `train_loader` which readily return mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "\n",
    "    data_file='SUSY.csv'\n",
    "    root_dir=os.path.expanduser('~')+'/ML_review/SUSY_data/'\n",
    "\n",
    "    kwargs = {} # CUDA arguments, if enabled\n",
    "    # load and noralise train and test data\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        SUSY_Dataset(data_file,root_dir,args.dataset_size,train=True,high_level_feats=args.high_level_feats),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        SUSY_Dataset(data_file,root_dir,args.dataset_size,train=False,high_level_feats=args.high_level_feats),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Neural Net and its Architecture\n",
    "\n",
    "To construct neural networks with Pytorch, we make another class called `model` as a child of Pytorch's `nn.Module` class. The `model` class initializes the types of layers needed for the deep neural net in its `__init__` method, while the DNN is assembled in a function method called `forward`, which accepts an `autograd.Variable` object and returns the output layer. Using this convention Pytorch will automatically recognize the structure of the DNN, and the `autograd` module will pull the gradients forward and backward using backprop.\n",
    "\n",
    "Our code below is constructed in such a way that one can choose whether to use the high-level and low-level features separately and altogether. This choice determines the size of the fully-connected input layer `fc1`. Therefore the `__init__` method accepts the optional argument `high_level_feats`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # construct NN\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self,high_level_feats=None):\n",
    "        # inherit attributes and methods of nn.Module\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        if high_level_feats is None:\n",
    "            self.fc1 = nn.Linear(18, 200) # all features\n",
    "        elif high_level_feats:\n",
    "            self.fc1 = nn.Linear(10, 200) # low-level only\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(8, 200) # high-level only\n",
    "\n",
    "\n",
    "        self.batchnorm1=nn.BatchNorm1d(200, eps=1e-05, momentum=0.1)\n",
    "        self.batchnorm2=nn.BatchNorm1d(100, eps=1e-05, momentum=0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(200, 100) # see forward function for dimensions\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Defines the feed-forward function for the NN.\n",
    "\n",
    "        A backward function is automatically defined using `torch.autograd`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : autograd.Tensor\n",
    "            input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        autograd.Tensor\n",
    "            output layer of NN\n",
    "\n",
    "        '''\n",
    "\n",
    "        # apply rectified linear unit\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # apply dropout\n",
    "        #x=self.batchnorm1(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "\n",
    "        # apply rectified linear unit\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # apply dropout\n",
    "        #x=self.batchnorm2(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "\n",
    "        # apply affine operation fc2\n",
    "        x = self.fc3(x)\n",
    "        # soft-max layer\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 3+4+5: Choose the Optimizer and the Cost Function. Train and Evaluate the Model\n",
    "\n",
    "Next, we define the function `evaluate_model`. The first argument, `args`, contains all hyperparameters needed for the DNN (see below). The second and third arguments are the `train_loader` and the `test_loader` objects, returned by the function `load_data()` we defined in Step 1 above. The `evaluate_model` function returns the final `test_loss` and `test_accuracy` of the model.\n",
    "\n",
    "First, we initialize a `model` and call the object `DNN`. In order to define the loss function and the optimizer, we use modules `torch.nn.functional` (imported here as `F`) and `torch.optim`. As a loss function we choose the negative log-likelihood, and stored is under the variable `criterion`. As usual, we can choose any from a variety of different SGD-based optimizers, but we focus on the traditional SGD.\n",
    "\n",
    "Next, we define two functions: `train()` and `test()`. They are called at the end of `evaluate_model` where we loop over the training epochs to train and test our model. \n",
    "\n",
    "The `train` function accepts an integer called `epoch`, which is only used to print the training data. We first set the `DNN` in a train mode using the `train()` method inherited from `nn.Module`. Then we loop over the mini-batches in `train_loader`. We cast the data as pytorch `Variable`, re-set the `optimizer`, perform the forward step by calling the `DNN` model on the `data` and computing the `loss`. The backprop algorithm is then easily done using the `backward()` method of the loss function `criterion`. We use `optimizer.step` to update the weights of the `DNN`. Last print the performance for every minibatch. `train` returns the loss on the data.\n",
    "\n",
    "The `test` function is similar to `train` but its purpose is to test the performance of a trained model. Once we set the `DNN` model in `eval()` mode, the following steps are similar to those in `train`. We then compute the `test_loss` and the number of `correct` predictions, print the results and return them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "import torch.optim as optim # different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc\n",
    "\n",
    "def evaluate_model(args,train_loader,test_loader):\n",
    "\n",
    "    # create model\n",
    "    DNN = model(high_level_feats=args.high_level_feats)\n",
    "    # negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n",
    "    criterion = F.nll_loss\n",
    "    # define SGD optimizer\n",
    "    optimizer = optim.SGD(DNN.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    #optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    def train(epoch):\n",
    "        '''Trains a NN using minibatches.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            Training epoch number.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # set model to training mode (affects Dropout and BatchNorm)\n",
    "        DNN.train()\n",
    "        # loop over training data\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "            label = label.type(torch.LongTensor)\n",
    "            # zero gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            # compute output of final layer: forward step\n",
    "            output = DNN(data)\n",
    "            # compute loss\n",
    "            loss = criterion(output, label)\n",
    "            # run backprop: backward step\n",
    "            loss.backward()\n",
    "            # update weigths of NN\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print loss at current epoch\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item() ))\n",
    "            \n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    def test():\n",
    "        '''Tests NN performance.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # evaluate model\n",
    "        DNN.eval()\n",
    "\n",
    "        test_loss = 0 # loss function on test data\n",
    "        correct = 0 # number of correct predictions\n",
    "        # loop over test data\n",
    "        for data, label in test_loader:\n",
    "            # compute model prediction softmax probability\n",
    "            label = label.type(torch.LongTensor)\n",
    "            output = DNN(data)\n",
    "            # compute test loss\n",
    "            test_loss += criterion(output, label, size_average=False).item() # sum up batch loss\n",
    "            # find most likely prediction\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            # update number of correct predictions\n",
    "            correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        # print test loss\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "\n",
    "        return test_loss, correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    train_loss=np.zeros((args.epochs,))\n",
    "    test_loss=np.zeros_like(train_loss)\n",
    "    test_accuracy=np.zeros_like(train_loss)\n",
    "\n",
    "    epochs=range(1, args.epochs + 1)\n",
    "    for epoch in epochs:\n",
    "\n",
    "        train_loss[epoch-1] = train(epoch)\n",
    "        test_loss[epoch-1], test_accuracy[epoch-1] = test()\n",
    "\n",
    "\n",
    "\n",
    "    return test_loss[-1], test_accuracy[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Modify the Hyperparameters to Optimize Performance of the Model\n",
    "\n",
    "To study the performance of the model for a variety of different `data_set_sizes` and `learning_rates`, we do a grid search. \n",
    "\n",
    "Let us define a function `grid_search`, which accepts the `args` variable containing all hyper-parameters needed for the problem. After choosing logarithmically-spaced `data_set_sizes` and `learning_rates`, we first loop over all `data_set_sizes`, update the `args` variable, and call the `load_data` function. We then loop once again over all `learning_rates`, update `args` and call `evaluate_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(args):\n",
    "\n",
    "\n",
    "    # perform grid search over learnign rate and number of hidden neurons\n",
    "    dataset_sizes=[1000, 10000, 100000, 200000] #np.logspace(2,5,4).astype('int')\n",
    "    learning_rates=np.logspace(-5,-1,5)\n",
    "\n",
    "    # pre-alocate data\n",
    "    test_loss=np.zeros((len(dataset_sizes),len(learning_rates)),dtype=np.float64)\n",
    "    test_accuracy=np.zeros_like(test_loss)\n",
    "\n",
    "    # do grid search\n",
    "    for i, dataset_size in enumerate(dataset_sizes):\n",
    "        # upate data set size parameters\n",
    "        args.dataset_size=dataset_size\n",
    "        args.batch_size=int(0.01*dataset_size)\n",
    "\n",
    "        # load data\n",
    "        train_loader, test_loader = load_data(args)\n",
    "\n",
    "        for j, lr in enumerate(learning_rates):\n",
    "            # update learning rate\n",
    "            args.lr=lr\n",
    "\n",
    "            print(\"\\n training DNN with %5d data points and SGD lr=%0.6f. \\n\" %(dataset_size,lr) )\n",
    "\n",
    "            test_loss[i,j],test_accuracy[i,j] = evaluate_model(args,train_loader,test_loader)\n",
    "\n",
    "\n",
    "    plot_data(learning_rates,dataset_sizes,test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we use the function `plot_data`, defined below, to plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(x,y,data):\n",
    "\n",
    "    # plot results\n",
    "    fontsize=16\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
    "    \n",
    "    cbar=fig.colorbar(cax)\n",
    "    cbar.ax.set_ylabel('accuracy (%)',rotation=90,fontsize=fontsize)\n",
    "    cbar.set_ticks([0,.2,.4,0.6,0.8,1.0])\n",
    "    cbar.set_ticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
    "\n",
    "    # convert axis vaues to to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\mathrm{learning\\\\ rate}$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\mathrm{hidden\\\\ neurons}$',fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Code\n",
    "\n",
    "As we mentioned in the beginning of the notebook, all functions and classes discussed above only specify the procedure but do not actually perform any computations. This allows us to re-use them for different problems. \n",
    "\n",
    "Actually running the training and testing for every point in the grid search is done below. The `argparse` class allows us to conveniently keep track of all hyperparameters, stored in the variable `args` which enters most of the functions we defined above. \n",
    "\n",
    "To run the simulation, we call the function `grid_search`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 800 examples\n",
      "Using both high and low level features\n",
      "Testing on 200 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.692516\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.815325\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.629332\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.630804\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.741583\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.569039\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.609717\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.683368\n",
      "\n",
      "Test set: Average loss: 0.6872, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.723292\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.724125\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.654607\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.696129\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.645358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yi Ming Chang\\anaconda3\\envs\\Tensflow\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.726247\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.711906\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.711763\n",
      "\n",
      "Test set: Average loss: 0.6869, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.759501\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.871239\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.614383\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.722174\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.741773\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.708904\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.702349\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.714223\n",
      "\n",
      "Test set: Average loss: 0.6865, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.609514\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.697677\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.765077\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.716423\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.687557\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.768412\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.664882\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.755054\n",
      "\n",
      "Test set: Average loss: 0.6862, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.743819\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.703166\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.669054\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.669098\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.834726\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.764498\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.689127\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.616188\n",
      "\n",
      "Test set: Average loss: 0.6859, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.700041\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.653740\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.617519\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.769483\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.571790\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.723388\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.737707\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.737847\n",
      "\n",
      "Test set: Average loss: 0.6856, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.858864\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.689336\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.779760\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.733956\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.801238\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.688367\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.701228\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.751082\n",
      "\n",
      "Test set: Average loss: 0.6853, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.719717\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.645818\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.741556\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.714371\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.686738\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.662365\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.617552\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.698682\n",
      "\n",
      "Test set: Average loss: 0.6850, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.711361\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.765464\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.709751\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.713927\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.674116\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.685336\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.788109\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.593868\n",
      "\n",
      "Test set: Average loss: 0.6847, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.664889\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.680259\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.707752\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.699693\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.721796\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.727530\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.621379\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.689134\n",
      "\n",
      "Test set: Average loss: 0.6844, Accuracy: 110/200 (55.000%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.691447\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.765696\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.731615\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.696125\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.658610\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.746519\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.684012\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.748131\n",
      "\n",
      "Test set: Average loss: 0.6867, Accuracy: 95/200 (47.500%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.739027\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.725723\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.655366\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.727852\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.696294\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.676779\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.732684\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.703295\n",
      "\n",
      "Test set: Average loss: 0.6853, Accuracy: 97/200 (48.500%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.692760\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.629498\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.669413\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.735529\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.701281\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.663598\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.717448\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.719136\n",
      "\n",
      "Test set: Average loss: 0.6842, Accuracy: 97/200 (48.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.672089\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.676892\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.694115\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.766470\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.719550\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.625484\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.696759\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.669436\n",
      "\n",
      "Test set: Average loss: 0.6829, Accuracy: 106/200 (53.000%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.714679\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.697520\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.739608\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.708526\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.660452\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.629330\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.729575\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.678115\n",
      "\n",
      "Test set: Average loss: 0.6818, Accuracy: 110/200 (55.000%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.678009\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.643932\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.691092\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.698361\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.729211\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.684240\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.658024\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.718513\n",
      "\n",
      "Test set: Average loss: 0.6807, Accuracy: 112/200 (56.000%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.724252\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.664642\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.668700\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.654960\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.728954\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.676448\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.685175\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.696858\n",
      "\n",
      "Test set: Average loss: 0.6798, Accuracy: 117/200 (58.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.701662\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.675762\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.686852\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.675394\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.692233\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.693899\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.662580\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.637687\n",
      "\n",
      "Test set: Average loss: 0.6788, Accuracy: 118/200 (59.000%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.701505\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.674740\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.621879\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.686088\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.664010\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.660244\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.688369\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.705213\n",
      "\n",
      "Test set: Average loss: 0.6778, Accuracy: 122/200 (61.000%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.681743\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.716596\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.647817\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.646180\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.790796\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.650597\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.720706\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.733348\n",
      "\n",
      "Test set: Average loss: 0.6769, Accuracy: 127/200 (63.500%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.731855\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.700704\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.716347\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.674262\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.751293\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.761984\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.742785\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.696057\n",
      "\n",
      "Test set: Average loss: 0.6861, Accuracy: 113/200 (56.500%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.651051\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.648303\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.715687\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.655465\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.723671\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.699465\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.746543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.705105\n",
      "\n",
      "Test set: Average loss: 0.6733, Accuracy: 135/200 (67.500%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.657141\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.737194\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.646873\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.694151\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.638729\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.690653\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.655748\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.598510\n",
      "\n",
      "Test set: Average loss: 0.6626, Accuracy: 137/200 (68.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.759147\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.660012\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.706267\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.664730\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.760567\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.702265\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.599655\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.683525\n",
      "\n",
      "Test set: Average loss: 0.6524, Accuracy: 143/200 (71.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.697384\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.750108\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.619613\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.636009\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.703776\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.671919\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.734879\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.623017\n",
      "\n",
      "Test set: Average loss: 0.6411, Accuracy: 141/200 (70.500%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.633831\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.583563\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.645034\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.635211\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.654564\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.669128\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.658380\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.637955\n",
      "\n",
      "Test set: Average loss: 0.6301, Accuracy: 146/200 (73.000%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.729083\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.660605\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.581679\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.656456\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.730629\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.622297\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.739138\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.641714\n",
      "\n",
      "Test set: Average loss: 0.6195, Accuracy: 151/200 (75.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.659264\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.669941\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.587695\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.679131\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.609355\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.576359\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.673183\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.575109\n",
      "\n",
      "Test set: Average loss: 0.6072, Accuracy: 151/200 (75.500%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.555550\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.724745\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.579653\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.604577\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.546886\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.733721\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.529229\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.629515\n",
      "\n",
      "Test set: Average loss: 0.5971, Accuracy: 150/200 (75.000%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.605406\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.512932\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.636070\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.599778\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.728604\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.613495\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.659553\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.565705\n",
      "\n",
      "Test set: Average loss: 0.5879, Accuracy: 152/200 (76.000%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.665003\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.676265\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.635049\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.624394\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.731200\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.639681\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.647450\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.750127\n",
      "\n",
      "Test set: Average loss: 0.6385, Accuracy: 132/200 (66.000%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.616201\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.591955\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.567527\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.683810\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.492011\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.663046\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.534449\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.657558\n",
      "\n",
      "Test set: Average loss: 0.5951, Accuracy: 132/200 (66.000%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.656008\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.584475\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.532299\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.452912\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.629123\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.496491\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.787437\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.690673\n",
      "\n",
      "Test set: Average loss: 0.5523, Accuracy: 146/200 (73.000%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.481523\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.531251\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.411792\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.588521\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.699045\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.499117\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.440411\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.629926\n",
      "\n",
      "Test set: Average loss: 0.5144, Accuracy: 153/200 (76.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.548161\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.375763\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.318504\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.587465\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.651538\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.428843\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.452427\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.323405\n",
      "\n",
      "Test set: Average loss: 0.5026, Accuracy: 153/200 (76.500%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.539850\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.409117\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.569787\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.435924\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.718353\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.334885\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.356753\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.417456\n",
      "\n",
      "Test set: Average loss: 0.4993, Accuracy: 155/200 (77.500%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.475706\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.545517\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.380334\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.442400\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.419722\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.688002\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.205493\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.677046\n",
      "\n",
      "Test set: Average loss: 0.4895, Accuracy: 155/200 (77.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.386303\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.295968\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.613414\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.386960\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.269213\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.323726\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.673769\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.423070\n",
      "\n",
      "Test set: Average loss: 0.5025, Accuracy: 150/200 (75.000%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.544513\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.619017\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.406950\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.325123\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.864314\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.383275\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.568506\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.571173\n",
      "\n",
      "Test set: Average loss: 0.4962, Accuracy: 152/200 (76.000%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.491004\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.590797\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.589814\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.148303\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.590936\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.375367\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.248993\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.288816\n",
      "\n",
      "Test set: Average loss: 0.4992, Accuracy: 157/200 (78.500%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.680683\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.636552\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.481399\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.836537\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.545884\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 1.112049\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.541610\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.581375\n",
      "\n",
      "Test set: Average loss: 0.5687, Accuracy: 143/200 (71.500%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.516718\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.310333\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.642578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.581882\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.557238\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.518515\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.982870\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.393156\n",
      "\n",
      "Test set: Average loss: 0.5337, Accuracy: 149/200 (74.500%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.655493\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.552519\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.545929\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.729085\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.782234\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.733550\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.564837\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.673136\n",
      "\n",
      "Test set: Average loss: 0.5833, Accuracy: 141/200 (70.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.846822\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.463495\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.542241\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.544542\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.823923\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.612028\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.553033\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.443206\n",
      "\n",
      "Test set: Average loss: 0.5605, Accuracy: 135/200 (67.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.789309\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.293699\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.694629\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.441359\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.719211\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.652799\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.457920\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.296935\n",
      "\n",
      "Test set: Average loss: 0.5468, Accuracy: 141/200 (70.500%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.622927\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.644131\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.728771\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.547802\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.464938\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.578540\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.723088\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.941011\n",
      "\n",
      "Test set: Average loss: 0.5686, Accuracy: 143/200 (71.500%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.812627\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.871031\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.518194\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.604837\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.441041\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.475284\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 1.170037\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.498268\n",
      "\n",
      "Test set: Average loss: 0.5484, Accuracy: 139/200 (69.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.464924\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.494615\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.181525\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.833291\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.412491\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.471535\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.631764\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.469684\n",
      "\n",
      "Test set: Average loss: 0.6089, Accuracy: 127/200 (63.500%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.478604\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.535982\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.413695\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.932073\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.557992\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.254769\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.601073\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.541568\n",
      "\n",
      "Test set: Average loss: 0.5377, Accuracy: 150/200 (75.000%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.592895\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.790669\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.598989\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.509656\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.666436\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.774302\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.457819\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.517956\n",
      "\n",
      "Test set: Average loss: 0.5582, Accuracy: 139/200 (69.500%)\n",
      "\n",
      "Training on 8000 examples\n",
      "Using both high and low level features\n",
      "Testing on 2000 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.691783\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.722603\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.713980\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.745019\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.719173\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.709640\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.720012\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.696562\n",
      "\n",
      "Test set: Average loss: 0.6902, Accuracy: 1042/2000 (52.100%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.698440\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.691870\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.685456\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.716462\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.699220\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.700720\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.701246\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.717591\n",
      "\n",
      "Test set: Average loss: 0.6900, Accuracy: 1044/2000 (52.200%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.696650\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.699920\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.710607\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.704937\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.677867\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.708487\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.696032\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.687521\n",
      "\n",
      "Test set: Average loss: 0.6898, Accuracy: 1048/2000 (52.400%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.701056\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.712115\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.718050\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.705286\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.685914\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.681809\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.691757\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.702477\n",
      "\n",
      "Test set: Average loss: 0.6896, Accuracy: 1048/2000 (52.400%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.734807\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.680703\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.692374\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.695423\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.718509\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.727789\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.677965\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.685720\n",
      "\n",
      "Test set: Average loss: 0.6894, Accuracy: 1052/2000 (52.600%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.697053\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.702753\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.743460\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.708346\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.696602\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.695376\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.680978\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.701118\n",
      "\n",
      "Test set: Average loss: 0.6892, Accuracy: 1056/2000 (52.800%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.692426\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.687917\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.715801\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.733196\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.715544\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.709972\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.698188\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.725542\n",
      "\n",
      "Test set: Average loss: 0.6891, Accuracy: 1056/2000 (52.800%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.674716\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.688013\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.682434\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.689339\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.712490\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.705552\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.745795\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.712036\n",
      "\n",
      "Test set: Average loss: 0.6889, Accuracy: 1061/2000 (53.050%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.693232\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.688570\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.710700\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.749231\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.681736\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.708239\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.702178\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.693533\n",
      "\n",
      "Test set: Average loss: 0.6887, Accuracy: 1061/2000 (53.050%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.696477\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.720863\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.751993\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.696586\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.698388\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.698252\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.678641\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.731590\n",
      "\n",
      "Test set: Average loss: 0.6885, Accuracy: 1064/2000 (53.200%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.711809\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.714795\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.667177\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.703012\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.694410\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.701250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.685212\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.699360\n",
      "\n",
      "Test set: Average loss: 0.6889, Accuracy: 968/2000 (48.400%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.731116\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.712136\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.693269\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.716051\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.718777\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.707093\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.712078\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.697812\n",
      "\n",
      "Test set: Average loss: 0.6877, Accuracy: 1007/2000 (50.350%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.705660\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.700941\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.717860\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.686820\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.714944\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.686908\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.695971\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.692962\n",
      "\n",
      "Test set: Average loss: 0.6865, Accuracy: 1042/2000 (52.100%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.700901\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.694463\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.704686\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.698469\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.682041\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.706428\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.698561\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.721210\n",
      "\n",
      "Test set: Average loss: 0.6854, Accuracy: 1085/2000 (54.250%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.685315\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.723368\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.687552\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.699595\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.708341\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.689807\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.714251\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.693586\n",
      "\n",
      "Test set: Average loss: 0.6843, Accuracy: 1131/2000 (56.550%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.690022\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.676179\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.701945\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.693413\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.678346\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.704387\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.699866\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.692839\n",
      "\n",
      "Test set: Average loss: 0.6834, Accuracy: 1156/2000 (57.800%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.700977\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.689616\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.684920\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.717882\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.695940\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.694271\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.704409\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.682280\n",
      "\n",
      "Test set: Average loss: 0.6823, Accuracy: 1192/2000 (59.600%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.698494\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.651676\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.700764\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.686577\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.682797\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.704429\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.694394\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.695042\n",
      "\n",
      "Test set: Average loss: 0.6811, Accuracy: 1211/2000 (60.550%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.694132\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.698715\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.677724\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.708689\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.702799\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.691548\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.700166\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.701455\n",
      "\n",
      "Test set: Average loss: 0.6801, Accuracy: 1227/2000 (61.350%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.680993\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.691593\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.685796\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.676179\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.679947\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.677801\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.704828\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.709849\n",
      "\n",
      "Test set: Average loss: 0.6790, Accuracy: 1253/2000 (62.650%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.736173\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.755509\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.701595\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.695490\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.697725\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.711409\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.677971\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.689903\n",
      "\n",
      "Test set: Average loss: 0.6893, Accuracy: 1115/2000 (55.750%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.731286\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.702542\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.679958\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.685998\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.703056\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.672982\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.682933\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.694621\n",
      "\n",
      "Test set: Average loss: 0.6791, Accuracy: 1257/2000 (62.850%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.690320\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.668862\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.698060\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.671058\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.678354\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.670539\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.689190\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.683278\n",
      "\n",
      "Test set: Average loss: 0.6695, Accuracy: 1346/2000 (67.300%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.696090\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.688956\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.678322\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.682611\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.684937\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.691046\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.669867\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.648704\n",
      "\n",
      "Test set: Average loss: 0.6606, Accuracy: 1391/2000 (69.550%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.663900\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.663637\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.665536\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.665056\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.679005\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.640683\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.648275\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.667103\n",
      "\n",
      "Test set: Average loss: 0.6522, Accuracy: 1411/2000 (70.550%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.660257\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.653072\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.665164\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.650649\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.683407\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.643491\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.634486\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.625881\n",
      "\n",
      "Test set: Average loss: 0.6439, Accuracy: 1430/2000 (71.500%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.684188\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.665046\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.638987\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.686772\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.667645\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.654822\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.649671\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.626948\n",
      "\n",
      "Test set: Average loss: 0.6349, Accuracy: 1454/2000 (72.700%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.613949\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.631976\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.647194\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.653824\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.654356\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.650990\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.641440\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.646450\n",
      "\n",
      "Test set: Average loss: 0.6267, Accuracy: 1471/2000 (73.550%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.655725\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.646600\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.644327\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.619724\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.611306\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.636783\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.633192\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.619377\n",
      "\n",
      "Test set: Average loss: 0.6188, Accuracy: 1473/2000 (73.650%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.635446\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.634642\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.633937\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.598953\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.643659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.615661\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.628712\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.616715\n",
      "\n",
      "Test set: Average loss: 0.6106, Accuracy: 1480/2000 (74.000%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.705221\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.690476\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.664106\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.644082\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.644636\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.670541\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.614270\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.623820\n",
      "\n",
      "Test set: Average loss: 0.6051, Accuracy: 1492/2000 (74.600%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.645210\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.648198\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.595712\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.620729\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.603311\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.623296\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.577130\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.579054\n",
      "\n",
      "Test set: Average loss: 0.5437, Accuracy: 1506/2000 (75.300%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.608649\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.601334\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.541450\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.557137\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.576424\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.562012\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.554565\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.497513\n",
      "\n",
      "Test set: Average loss: 0.5091, Accuracy: 1532/2000 (76.600%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.535878\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.529306\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.571366\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.501691\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.501165\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.528977\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.526550\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.617593\n",
      "\n",
      "Test set: Average loss: 0.4914, Accuracy: 1554/2000 (77.700%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.498848\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.445073\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.537681\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.552003\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.601010\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.480264\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.629385\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.478942\n",
      "\n",
      "Test set: Average loss: 0.4807, Accuracy: 1561/2000 (78.050%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.397125\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.474372\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.472885\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.514604\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.496206\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.514751\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.541955\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.548586\n",
      "\n",
      "Test set: Average loss: 0.4780, Accuracy: 1563/2000 (78.150%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.555204\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.453748\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.590327\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.496093\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.512905\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.484890\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.509143\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.523437\n",
      "\n",
      "Test set: Average loss: 0.4747, Accuracy: 1544/2000 (77.200%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.495124\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.504034\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.597897\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.545713\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.441280\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.547938\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.481709\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.533116\n",
      "\n",
      "Test set: Average loss: 0.4722, Accuracy: 1546/2000 (77.300%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.513619\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.472225\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.525754\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.433016\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.464887\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.520713\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.499928\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.447477\n",
      "\n",
      "Test set: Average loss: 0.4617, Accuracy: 1591/2000 (79.550%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.409937\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.475376\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.492837\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.464670\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.469587\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.421717\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.486610\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.453443\n",
      "\n",
      "Test set: Average loss: 0.4631, Accuracy: 1579/2000 (78.950%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.692121\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.636557\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.592680\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.576478\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.533510\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.541482\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.440173\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.464656\n",
      "\n",
      "Test set: Average loss: 0.4765, Accuracy: 1550/2000 (77.500%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.471349\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.527443\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.552770\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.541026\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.473833\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.554425\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.435723\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.492694\n",
      "\n",
      "Test set: Average loss: 0.4647, Accuracy: 1576/2000 (78.800%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.469619\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.435790\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.525032\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.511919\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.368812\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.482286\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.432491\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.637639\n",
      "\n",
      "Test set: Average loss: 0.4582, Accuracy: 1580/2000 (79.000%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.443136\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.492567\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.475573\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.379322\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.356851\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.417378\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.446635\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.444207\n",
      "\n",
      "Test set: Average loss: 0.4564, Accuracy: 1594/2000 (79.700%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.626505\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.396748\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.538140\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.454365\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.471114\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.394999\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.561054\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.563204\n",
      "\n",
      "Test set: Average loss: 0.4533, Accuracy: 1583/2000 (79.150%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.590324\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.547440\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.348742\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.441970\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.467704\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.436196\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.433980\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.518813\n",
      "\n",
      "Test set: Average loss: 0.4520, Accuracy: 1589/2000 (79.450%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.459125\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.445201\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.371699\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.470859\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.424638\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.476229\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.465642\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.507868\n",
      "\n",
      "Test set: Average loss: 0.4486, Accuracy: 1587/2000 (79.350%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.438039\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.415284\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.402204\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.464349\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.546327\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.508674\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.436179\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.433712\n",
      "\n",
      "Test set: Average loss: 0.4534, Accuracy: 1564/2000 (78.200%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.484956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.440864\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.491563\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.541206\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.525596\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.483802\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.493943\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.494872\n",
      "\n",
      "Test set: Average loss: 0.4549, Accuracy: 1582/2000 (79.100%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.491272\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.393417\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.428397\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.436683\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.378823\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.521724\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.394703\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.495529\n",
      "\n",
      "Test set: Average loss: 0.4485, Accuracy: 1599/2000 (79.950%)\n",
      "\n",
      "Training on 80000 examples\n",
      "Using both high and low level features\n",
      "Testing on 20000 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.708734\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.701799\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.702327\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.706047\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.708362\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.696628\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.700845\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.708652\n",
      "\n",
      "Test set: Average loss: 0.6992, Accuracy: 9053/20000 (45.265%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.706663\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.705031\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.701241\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.711710\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.709673\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.713435\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.702385\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.703694\n",
      "\n",
      "Test set: Average loss: 0.6991, Accuracy: 9061/20000 (45.305%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.705774\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.705166\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.705994\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.703305\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.703977\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.706398\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.704072\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.705442\n",
      "\n",
      "Test set: Average loss: 0.6990, Accuracy: 9072/20000 (45.360%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.705183\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.707720\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.710405\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.707023\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.699029\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.709600\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.708877\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.710930\n",
      "\n",
      "Test set: Average loss: 0.6988, Accuracy: 9085/20000 (45.425%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.699199\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.715007\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.712017\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.698083\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.706671\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.700428\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.701904\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.703967\n",
      "\n",
      "Test set: Average loss: 0.6987, Accuracy: 9100/20000 (45.500%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.709439\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.701525\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.705485\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.696956\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.697424\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.700869\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.705837\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.701543\n",
      "\n",
      "Test set: Average loss: 0.6986, Accuracy: 9112/20000 (45.560%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.705333\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.700340\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.700864\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.703463\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.706103\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.707143\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.703781\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.707849\n",
      "\n",
      "Test set: Average loss: 0.6984, Accuracy: 9124/20000 (45.620%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.705776\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.696711\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.703593\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.702612\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.709079\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.697700\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.707334\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.709424\n",
      "\n",
      "Test set: Average loss: 0.6983, Accuracy: 9136/20000 (45.680%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.704532\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.699780\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.698006\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.688352\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.701253\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.702153\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.701580\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.703244\n",
      "\n",
      "Test set: Average loss: 0.6982, Accuracy: 9153/20000 (45.765%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.701148\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.704454\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.701974\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.703533\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.712506\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.699732\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.705841\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.706133\n",
      "\n",
      "Test set: Average loss: 0.6980, Accuracy: 9171/20000 (45.855%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.709153\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.708617\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.706155\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.704764\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.703673\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.707853\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.708016\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.708748\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 9269/20000 (46.345%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.698873\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.704494\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.701453\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.699175\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.707037\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.705185\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.707942\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.703423\n",
      "\n",
      "Test set: Average loss: 0.6980, Accuracy: 9351/20000 (46.755%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.709141\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.699590\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.706395\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.701718\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.706010\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.709439\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.706501\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.704986\n",
      "\n",
      "Test set: Average loss: 0.6967, Accuracy: 9456/20000 (47.280%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.707645\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.698514\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.711741\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.698031\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.700956\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.703356\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.704844\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.700520\n",
      "\n",
      "Test set: Average loss: 0.6954, Accuracy: 9560/20000 (47.800%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.701944\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.705776\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.703180\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.708968\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.702572\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.691205\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.703188\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.707042\n",
      "\n",
      "Test set: Average loss: 0.6942, Accuracy: 9700/20000 (48.500%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.698627\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.702757\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.698480\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.702116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.699866\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.699751\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.699520\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.706300\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 9855/20000 (49.275%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.701067\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.692692\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.704309\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.705689\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.701595\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.692886\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.695105\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.700298\n",
      "\n",
      "Test set: Average loss: 0.6920, Accuracy: 9976/20000 (49.880%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.702606\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.702032\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.697804\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.696060\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.696311\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.707092\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.697639\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.702254\n",
      "\n",
      "Test set: Average loss: 0.6908, Accuracy: 10100/20000 (50.500%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.697367\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.696568\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.697266\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.692873\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.692427\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.693027\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.697361\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.696458\n",
      "\n",
      "Test set: Average loss: 0.6898, Accuracy: 10245/20000 (51.225%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.700497\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.695559\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.702267\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.696777\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.694041\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.694584\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.691061\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.697932\n",
      "\n",
      "Test set: Average loss: 0.6887, Accuracy: 10390/20000 (51.950%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.695070\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.692270\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.697036\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.693422\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.686493\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.690353\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.687380\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.685690\n",
      "\n",
      "Test set: Average loss: 0.6819, Accuracy: 12170/20000 (60.850%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.692299\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.681239\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.689888\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.686197\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.686881\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.681133\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.676703\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.678783\n",
      "\n",
      "Test set: Average loss: 0.6730, Accuracy: 13327/20000 (66.635%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.680271\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.680058\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.679625\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.670104\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.676781\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.675365\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.669390\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.668323\n",
      "\n",
      "Test set: Average loss: 0.6646, Accuracy: 13942/20000 (69.710%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.676008\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.668142\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.670434\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.662831\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.669126\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.662520\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.663127\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.664790\n",
      "\n",
      "Test set: Average loss: 0.6563, Accuracy: 14273/20000 (71.365%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.660308\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.668632\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.663279\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.660856\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.656199\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.665770\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.662613\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.654939\n",
      "\n",
      "Test set: Average loss: 0.6481, Accuracy: 14481/20000 (72.405%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.650535\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.662726\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.651652\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.658275\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.650486\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.650921\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.651700\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.652946\n",
      "\n",
      "Test set: Average loss: 0.6399, Accuracy: 14602/20000 (73.010%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.656022\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.645624\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.652607\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.652717\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.647871\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.654060\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.657022\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.649093\n",
      "\n",
      "Test set: Average loss: 0.6320, Accuracy: 14684/20000 (73.420%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.648224\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.643196\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.642013\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.641009\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.636941\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.635959\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.648351\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.637579\n",
      "\n",
      "Test set: Average loss: 0.6240, Accuracy: 14727/20000 (73.635%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.637899\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.640595\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.632816\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.652487\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.630284\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.637148\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.643450\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.639602\n",
      "\n",
      "Test set: Average loss: 0.6164, Accuracy: 14785/20000 (73.925%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.639600\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.626104\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.629268\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.624588\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.632334\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.631188\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.625427\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.627939\n",
      "\n",
      "Test set: Average loss: 0.6088, Accuracy: 14831/20000 (74.155%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.712830\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.692431\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.688743\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.670153\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.661575\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.652717\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.645833\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.631783\n",
      "\n",
      "Test set: Average loss: 0.6117, Accuracy: 14948/20000 (74.740%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.635990\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.619937\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.624587\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.610288\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.616121\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.600921\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.588354\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.592272\n",
      "\n",
      "Test set: Average loss: 0.5443, Accuracy: 15326/20000 (76.630%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.570779\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.565329\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.583568\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.569601\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.557004\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.571213\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.575575\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.557202\n",
      "\n",
      "Test set: Average loss: 0.5077, Accuracy: 15504/20000 (77.520%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.556287\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.565436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.552522\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.534412\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.578052\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.544910\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.541883\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.542347\n",
      "\n",
      "Test set: Average loss: 0.4893, Accuracy: 15580/20000 (77.900%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.538725\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.540356\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.509841\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.553458\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.525666\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.540351\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.536524\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.526711\n",
      "\n",
      "Test set: Average loss: 0.4798, Accuracy: 15583/20000 (77.915%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.520037\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.524950\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.525589\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.496618\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.499492\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.508261\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.509884\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.507416\n",
      "\n",
      "Test set: Average loss: 0.4735, Accuracy: 15643/20000 (78.215%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.489447\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.517629\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.501453\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.496427\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.483338\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.530601\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.518389\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.498489\n",
      "\n",
      "Test set: Average loss: 0.4695, Accuracy: 15665/20000 (78.325%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.521411\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.486164\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.486428\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.517282\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.491024\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.517534\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.493593\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.492632\n",
      "\n",
      "Test set: Average loss: 0.4656, Accuracy: 15711/20000 (78.555%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.500297\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.492546\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.496542\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.494181\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.506516\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.494013\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.493168\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.501402\n",
      "\n",
      "Test set: Average loss: 0.4630, Accuracy: 15719/20000 (78.595%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.512453\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.501088\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.473573\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.512234\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.489346\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.503823\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.466114\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.491342\n",
      "\n",
      "Test set: Average loss: 0.4613, Accuracy: 15748/20000 (78.740%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.709090\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.643826\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.593336\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.578081\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.535861\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.523428\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.523546\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.522533\n",
      "\n",
      "Test set: Average loss: 0.4719, Accuracy: 15584/20000 (77.920%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.482891\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.480546\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.485701\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.487946\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.492151\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.489757\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.452093\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.489009\n",
      "\n",
      "Test set: Average loss: 0.4627, Accuracy: 15615/20000 (78.075%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.465816\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.454966\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.480239\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.468276\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.471205\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.475914\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.454410\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.455322\n",
      "\n",
      "Test set: Average loss: 0.4510, Accuracy: 15860/20000 (79.300%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.470759\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.470219\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.470964\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.462279\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.453819\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.427923\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.480852\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.463165\n",
      "\n",
      "Test set: Average loss: 0.4496, Accuracy: 15881/20000 (79.405%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.453958\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.474777\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.457270\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.454926\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.455076\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.444762\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.467353\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.433821\n",
      "\n",
      "Test set: Average loss: 0.4468, Accuracy: 15897/20000 (79.485%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.428683\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.453573\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.444086\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.460859\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.441066\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.464862\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.441153\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.459527\n",
      "\n",
      "Test set: Average loss: 0.4473, Accuracy: 15867/20000 (79.335%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.459070\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.477708\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.439774\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.452503\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.459175\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.438286\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.467389\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.437061\n",
      "\n",
      "Test set: Average loss: 0.4455, Accuracy: 15916/20000 (79.580%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.447404\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.456532\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.451012\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.464708\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.450425\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.471467\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.462170\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.456746\n",
      "\n",
      "Test set: Average loss: 0.4451, Accuracy: 15898/20000 (79.490%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.464442\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.435708\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.461780\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.452562\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.478972\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.479933\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.463154\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.445677\n",
      "\n",
      "Test set: Average loss: 0.4441, Accuracy: 15903/20000 (79.515%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.459042\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.468664\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.452413\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.452196\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.442217\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.459677\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.456779\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.435136\n",
      "\n",
      "Test set: Average loss: 0.4453, Accuracy: 15886/20000 (79.430%)\n",
      "\n",
      "Training on 160000 examples\n",
      "Using both high and low level features\n",
      "Testing on 40000 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.714920\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.706031\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.703811\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.713812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.707214\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.708699\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.711472\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.710336\n",
      "\n",
      "Test set: Average loss: 0.7012, Accuracy: 17875/40000 (44.688%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.714116\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.706812\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.715721\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.706757\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.715657\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.710475\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.716585\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.707948\n",
      "\n",
      "Test set: Average loss: 0.7010, Accuracy: 17913/40000 (44.782%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.709716\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.713480\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.708743\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.717675\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.713171\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.714338\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.720692\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.711529\n",
      "\n",
      "Test set: Average loss: 0.7008, Accuracy: 17985/40000 (44.962%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.716364\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.712814\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.709754\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.705435\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.711248\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.711868\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.711759\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.712501\n",
      "\n",
      "Test set: Average loss: 0.7006, Accuracy: 18023/40000 (45.057%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.707871\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.706090\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.708286\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.707358\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.710403\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.709194\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.705991\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.710545\n",
      "\n",
      "Test set: Average loss: 0.7004, Accuracy: 18078/40000 (45.195%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.708145\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.712486\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.714200\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.711063\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.709119\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.711491\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.707892\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.709862\n",
      "\n",
      "Test set: Average loss: 0.7002, Accuracy: 18111/40000 (45.278%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.709945\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.712807\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.710523\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.707751\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.706535\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.716562\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.712070\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.713905\n",
      "\n",
      "Test set: Average loss: 0.7000, Accuracy: 18143/40000 (45.358%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.712204\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.717247\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.706506\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.713714\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.708842\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.711443\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.714141\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.717889\n",
      "\n",
      "Test set: Average loss: 0.6998, Accuracy: 18190/40000 (45.475%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.716786\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.710424\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.707609\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.712734\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.706319\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.707522\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.705027\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.703848\n",
      "\n",
      "Test set: Average loss: 0.6996, Accuracy: 18221/40000 (45.553%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.710996\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.715070\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.708006\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.710080\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.711452\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.712172\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.707718\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.711616\n",
      "\n",
      "Test set: Average loss: 0.6994, Accuracy: 18264/40000 (45.660%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.702009\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.696182\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.700131\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.705316\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.698824\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.704534\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.701035\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.702474\n",
      "\n",
      "Test set: Average loss: 0.6917, Accuracy: 21028/40000 (52.570%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.705714\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.699773\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.699366\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.697791\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.699740\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.699135\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.696057\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.697593\n",
      "\n",
      "Test set: Average loss: 0.6901, Accuracy: 21772/40000 (54.430%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.701420\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.698539\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.694682\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.695558\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.696155\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.697946\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.695099\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.703595\n",
      "\n",
      "Test set: Average loss: 0.6886, Accuracy: 22451/40000 (56.127%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.697659\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.698267\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.694096\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.696053\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.700172\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.693280\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.692932\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.693736\n",
      "\n",
      "Test set: Average loss: 0.6872, Accuracy: 23027/40000 (57.568%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.693236\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.691474\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.690235\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.695425\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.697169\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.697161\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.692241\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.695534\n",
      "\n",
      "Test set: Average loss: 0.6858, Accuracy: 23481/40000 (58.703%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.690138\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.692810\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.691924\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.692096\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.692840\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.694374\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.688815\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.690938\n",
      "\n",
      "Test set: Average loss: 0.6846, Accuracy: 23841/40000 (59.602%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.691238\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.692355\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.693143\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.690862\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.690197\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.695881\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.690879\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.691813\n",
      "\n",
      "Test set: Average loss: 0.6834, Accuracy: 24093/40000 (60.233%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.692962\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.686978\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.692753\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.694690\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.694828\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.690547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.688305\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.691522\n",
      "\n",
      "Test set: Average loss: 0.6822, Accuracy: 24314/40000 (60.785%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.694108\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.692381\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.693806\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.689660\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.692386\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.684599\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.691083\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.687539\n",
      "\n",
      "Test set: Average loss: 0.6811, Accuracy: 24520/40000 (61.300%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.692793\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.684640\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.688409\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.689059\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.686158\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.693805\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.687837\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.691365\n",
      "\n",
      "Test set: Average loss: 0.6800, Accuracy: 24733/40000 (61.833%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.714034\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.704149\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.696172\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.694816\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.685301\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.686646\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.681482\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.685930\n",
      "\n",
      "Test set: Average loss: 0.6717, Accuracy: 26443/40000 (66.108%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.677422\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.677015\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.678763\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.681143\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.680471\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.678577\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.668312\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.670035\n",
      "\n",
      "Test set: Average loss: 0.6602, Accuracy: 26724/40000 (66.810%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.668965\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.669154\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.665800\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.663510\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.671499\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.672960\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.658823\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.662844\n",
      "\n",
      "Test set: Average loss: 0.6498, Accuracy: 27886/40000 (69.715%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.666109\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.658549\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.659718\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.652975\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.649961\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.655587\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.652982\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.654808\n",
      "\n",
      "Test set: Average loss: 0.6399, Accuracy: 28577/40000 (71.442%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.646030\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.656250\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.653887\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.654189\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.651638\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.648476\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.644699\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.646094\n",
      "\n",
      "Test set: Average loss: 0.6306, Accuracy: 28975/40000 (72.438%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.642580\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.638493\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.644746\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.638711\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.644082\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.641076\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.641394\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.635009\n",
      "\n",
      "Test set: Average loss: 0.6214, Accuracy: 29234/40000 (73.085%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.636854\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.635783\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.636285\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.633927\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.636546\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.639404\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.639929\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.631908\n",
      "\n",
      "Test set: Average loss: 0.6125, Accuracy: 29461/40000 (73.653%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.629405\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.631885\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.627814\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.628678\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.632043\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.636328\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.622761\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.623693\n",
      "\n",
      "Test set: Average loss: 0.6041, Accuracy: 29609/40000 (74.022%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.624294\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.625246\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.627533\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.629950\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.623986\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.624028\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.624039\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.620790\n",
      "\n",
      "Test set: Average loss: 0.5960, Accuracy: 29720/40000 (74.300%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.622740\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.626016\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.629654\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.614119\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.618937\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.611150\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.613614\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.615392\n",
      "\n",
      "Test set: Average loss: 0.5884, Accuracy: 29853/40000 (74.632%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.709275\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.698408\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.688236\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.683237\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.674599\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.669146\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.662204\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.656372\n",
      "\n",
      "Test set: Average loss: 0.6360, Accuracy: 28335/40000 (70.838%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.649223\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.640317\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.630042\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.626536\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.620220\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.609673\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.601272\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.604048\n",
      "\n",
      "Test set: Average loss: 0.5662, Accuracy: 30046/40000 (75.115%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.595566\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.596923\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.586299\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.582571\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.568080\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.568802\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.563420\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.577389\n",
      "\n",
      "Test set: Average loss: 0.5201, Accuracy: 30631/40000 (76.578%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.557401\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.574589\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.547838\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.558078\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.545997\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.555394\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.543151\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.540274\n",
      "\n",
      "Test set: Average loss: 0.4947, Accuracy: 30922/40000 (77.305%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.548507\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.539934\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.542890\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.540733\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.528797\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.530201\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.529008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.535172\n",
      "\n",
      "Test set: Average loss: 0.4811, Accuracy: 31134/40000 (77.835%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.523800\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.529916\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.528916\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.524420\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.529820\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.522578\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.526922\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.512284\n",
      "\n",
      "Test set: Average loss: 0.4736, Accuracy: 31211/40000 (78.028%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.497731\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.522930\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.523327\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.510459\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.528738\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.509354\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.503957\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.512406\n",
      "\n",
      "Test set: Average loss: 0.4680, Accuracy: 31314/40000 (78.285%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.521150\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.496829\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.509812\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.492568\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.486878\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.493459\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.486415\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.495740\n",
      "\n",
      "Test set: Average loss: 0.4633, Accuracy: 31432/40000 (78.580%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.539229\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.514280\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.499585\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.513599\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.499697\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.513241\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.473917\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.503810\n",
      "\n",
      "Test set: Average loss: 0.4591, Accuracy: 31532/40000 (78.830%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.492126\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.517909\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.507471\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.482001\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.509713\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.499239\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.482106\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.497113\n",
      "\n",
      "Test set: Average loss: 0.4588, Accuracy: 31507/40000 (78.767%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.705480\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.636578\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.572808\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.553859\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.548325\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.513757\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.509080\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.492125\n",
      "\n",
      "Test set: Average loss: 0.4658, Accuracy: 31178/40000 (77.945%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.514144\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.491212\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.501645\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.473826\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.498489\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.468949\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.469516\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.471764\n",
      "\n",
      "Test set: Average loss: 0.4470, Accuracy: 31764/40000 (79.410%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.468978\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.473725\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.468594\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.480398\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.463952\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.458348\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.481503\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.477647\n",
      "\n",
      "Test set: Average loss: 0.4426, Accuracy: 31865/40000 (79.662%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.452518\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.452105\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.458901\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.478015\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.456067\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.464075\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.478783\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.471298\n",
      "\n",
      "Test set: Average loss: 0.4408, Accuracy: 31860/40000 (79.650%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.454080\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.468291\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.448248\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.473091\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.482343\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.466563\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.463745\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.476177\n",
      "\n",
      "Test set: Average loss: 0.4376, Accuracy: 31967/40000 (79.918%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.441204\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.480364\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.452073\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.460170\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.475512\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.447286\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.464683\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.459388\n",
      "\n",
      "Test set: Average loss: 0.4360, Accuracy: 31945/40000 (79.862%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.459185\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.448377\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.471876\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.456051\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.445679\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.454617\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.436093\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.446788\n",
      "\n",
      "Test set: Average loss: 0.4349, Accuracy: 31980/40000 (79.950%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.440024\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.477623\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.446100\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.464224\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.453409\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.460019\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.448827\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.444532\n",
      "\n",
      "Test set: Average loss: 0.4340, Accuracy: 31998/40000 (79.995%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.464022\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.426301\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.439923\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.441691\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.438455\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.456227\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.458038\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.445766\n",
      "\n",
      "Test set: Average loss: 0.4336, Accuracy: 31982/40000 (79.955%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.436997\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.447746\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.468472\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.432190\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.442439\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.462928\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.451520\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.476316\n",
      "\n",
      "Test set: Average loss: 0.4336, Accuracy: 32010/40000 (80.025%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-915f7288301d>:29: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(['']+x)\n",
      "<ipython-input-7-915f7288301d>:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(['']+y)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEJCAYAAADFB2O2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABbQElEQVR4nO3dd5xU1dnA8d+zvVd2F5al9y5VEFEBpdhLLLERozFRjDXWGFsSNXmTGBNNIvYawYhdwY50EJWqSG8LLJ3tuzPzvH/cu7DLthmYYWB5vn7ux5k795w5Z9mdZ06554iqYowxxhypIsJdAGOMMaYhFqiMMcYc0SxQGWOMOaJZoDLGGHNEs0BljDHmiGaByhhjzBHNAlWIiMhzIlIgIksOIm1/EVksIitF5B8iIu75n4nINhH5zj2uCX7Ja5VljIgsd8tyVx2vi1vGlSKySET6NZZWRDJE5BMRWeH+P909nykiX4hIkYg8Eeq6hbB+F4rIUhHxiciAw1GP+hxi/Q76d/hw8aN+XUVktoiUi8hvwlFGEwSqakcIDuAkoB+w5CDSzgOGAAJ8BIx1z/8MeOIw1iESWAW0B2KAhUD3A6453S2jAIOBuY2lBf4M3OU+vgv4k/s4ETgR+NXhqGcI69cN6AJ8CQwI4+/gQdfvUH+Hj6D6ZQMDgT8Cvwl3me04uMNaVCGiql8BO6ufE5EOIjJFRBaIyHQR6XpgOhFpAaSo6mx1/tJeAs49LIWubRCwUlVXq2oF8DpwzgHXnAO8pI45QJpbh4bSngO86D5+Ebd+qlqsqjOAslBWqpqQ1E9Vv1fV5YepDg05lPrV+Tt8hGm0fqpaoKrzgcpwFNAEhwWqw2sC8GtV7Q/8BvhXHde0BDZWe77RPVflAreL5n8i0ip0Rd1Xlg0NlKWhaxpKm6OqmwHc/2cHscyBCFX9jhSHUr+jwdFcdhMAC1SHiYgkAScAb4jId8BTQIu6Lq3jXNU6V+8BbVW1N/Ap+1slodJQWRq7xp+04Wb1OzrqUZ+juewhU9fYYn3jwu5rd7tjfMtFZLR7Ltbt/VkiItdXu3aCiPQ9vDWyQHU4RQC7VfW4akc3EYmsNjniIZxvhXnV0uUB+QCqukNVy93zTwP9Q1zmjUD1Vtu+svhxTUNpt1Z1L7n/LwhimQMRqvodKQ6lfkeDo7nsofQCMOaAc3cBn6lqJ+Az9zki0h24BOjhpvmXiEQCo4EFQG/gWvfaPkCEqn57GOpQgwWqw0RV9wJrRORC2Dfbqo+qeqsFrvvcrrBCERnszva7EnjHTVO9BXY28H2Iiz0f6CQi7UQkBucX+t0DrnkXuNKtz2Bgj1uHhtK+C4xzH4/DrV8YhKp+R4pDqd/R4Gj4Nzjs6hlbrHNc2D3/uqqWq+oaYCXO2F8lEA9EVcvj98B9ISp2g6Iav8QcDBH5L3AK0ExENgL3A5cB/xaRe4FonMHfhXUkvw7nW1E8zoysj9zzN4rI2YAH5xfxZ6GrAaiqR0RuAKbizLB6TlWXisiv3Nf/A3yIM3NsJVACXNVQWjfrR4FJInI1sB64sOo9RWQtkALEiMi5wChVXXY01U9EzgP+CWQBH4jId6o6OhR1aMih1A/q/h1W1WcPby3q50/9RKQ58DXO75RPRG7GmRm4N1zlPtDo4Ym6Y6fXr2sXLCpfSs3JRhNUdYIfSWuMC4tI1bhwS2BOteuqxvneBq4A5gJ/dj93FqhqWFqs4kwsM8YYEw4D+sTpvKmt/bo2ssWKBara6L15ItIWeF9Ve7rPd6tqWrXXd6lquog8CcxW1Vfc888CH6rqm9Wujcb5MnA28BDQGmem6GFrvVrXnzHGhJECPj//OwT1jQv7M853PU534RCgArgYuPdQChMoC1TGGBNWild9fh2HoL5x4XeBS9xZfu2ATjgLDgDgzg48E+d+zgTAhxNb4w6lMIGyMSpjjAkjp0UVvCGYesbH6xwXdsf0JgHLcMa+x6tq9QGz+4A/qKqKyFRgPLAY+E/QCuwHG6Myxpgw6tsnRqd91Nyva1NbbvBrjKqpsRaVMcaEWTBbVE2RjVEdQUTk2nCXIdiaWp2aWn3A6hRuCnhRv45jlQWqI8tR88cVgKZWp6ZWH7A6hZ0P9es4VlnXnzHGhJECXpsr0CALVI2ITE7UqMz0xi8MxntlpBHbNi/kv7ERUYc0zTUg0VmpxHfMDXmdEqMrQv0Wzvs0T6RZt2Yhr0+kHL5/o+TmCeR0zwh5nUq8MaF+i31is5NJ7tI85HUq+nHrdlXNOtR8Dt+/9tHJAlUjojLTaX7vjeEuRlAlZRWHuwhBNzh3XbiLEFQpUaXhLkLQfbcrr/GLjjJfjvzbIf/i6TE+/uQPC1TGGBNOCl6LUw2yQGWMMWGkCJV1bq1lqligMsaYMFLAZy2qBlmgMsaYMPNai6pBFqiMMSaMnBt+LVA1xAKVMcaEmU8tUDXEApUxxoSRtagaZ4HKGGPCSBG8tppdgyxQGWNMmFnXX8MsUBljTBhZ11/jLFAZY0wYKUKl2kdxQ+ynY4wxYWYtqoZZoDLGmDBSFbxqkykaYoHKGGPCzGctqgZZoDLGmDByJlNYi6ohFqiMMSasrOuvMRaojDEmjBTwWYuqQRaojDEmzLx2w2+DLFAZY0wY2RJKjbNAZYwxYaRgN/w2wn46xhgTRopY118jjopAJSLPAWcCBara0z2XAUwE2gJrgYtUdZf72t3A1YAXuFFVp7rn+wMvAPHAh8BNqhrUTaA33fUoEXGxIIJERtD83hsbPF9d6ZLl7Hr9XfApicMGkjp2OADewiK2/+tlfCWlpJ47moS+PQDY9sSLpF9+HlFpKcGsQi3eojLyn3yP8vUFIELuDWcR174Fa3/7AlrpBa+P5BO6kf3TU2qlXfGLx4mIj4UIp97t//oLADx7itnw6CR8xWVkXTqclMFdAVj/8Ou0+NUZRGckh6w+e9bt4Yt7pu17XphfRL9rj6PHT7tTXljBzD/OYteqXSDCsHtPILt3do30G2dvYs5f56E+pfM5negzrhcApbvK+OyOL6gorKD/r/rS5pTWAHz6m8854c7BJGQlhKxOO9fu5cO75ux7vndTEYN/1ZN+l3Xmm1eWs+TtNYhAZsdURj0wiKjYyBrpnz3jfWISo5EIISJSuPTV0wAo2VXG+7fNorywgiHX96Lj8JYAvHvLDEbc05+krPiQ1amyqIzlf/mE4rXbERG6/GYUqT1y2fjmN+R/uBgUWpzRi1YX9KuVdvalzxCVEA0REUhkBAP+fRkAFbtLWHL/u3iKyml31VCyTuwIwOLfvUPnm0YS2ywpZPVpiE2maNhREahwgssTwEvVzt0FfKaqj4rIXe7zO0WkO3AJ0APIBT4Vkc6q6gX+DVwLzMEJVGOAj4Jd2OzbriUyOdHv8wDq87HrtbfJvuUaItNT2fLHJ0jo053o3BxK5i0kcUg/EgYdR8HfnyWhbw9KFi4jpnVuyIMUwJZnp5DUrwOt7rwQrfTiK69EoiNp+9CVRMTHoB4va+5+nqR+HUnoklcrfZs/XElUSs0P6T3Tl5A2vA8pw3qw/sFXSRnclcJ5y4lv3yKkQQogtU0q5756NgA+r4+JZ7yxL6jM/es8Wg7OZcSjp+Ct9OIp89ZI6/P6mP3nOYx+YhSJ2Qm8O+4DWg9rRXr7NFZ/vIZOZ3Sg/WntmHrTJ7Q5pTXrp28gs0tGSIMUQEbbFC5/fdS+Mj4z5n06Dm9JUUEJ372+kiv/N5qouCg+uHMWy6eup8fZ7Wrl8ZOnTiE+PbbGueVT1tPtzDZ0Gd2at274io7DW7J6Wj7ZXdNDGqQAVj7xJRkD29LzgbPwVXrxlldStGY7+R8upv+TlyLRkSy6azKZx7cjIS+9Vvo+f72ImNSaZSz4/Aeaj+pO9vCuLLprMlkndmT7rFUkd8oOW5BSxaanN+Ko+Omo6lfAzgNOnwO86D5+ETi32vnXVbVcVdcAK4FBItICSFHV2W4r6qVqacKuYs0GorIyicrKRKKiSBjYh5LvljkvRkaglR600oOIoF4vhZ/OIHn0ySEvl7eknJKl60k7tS8AEh1JZFIcIkJEfAwA6vWB10cgN9dLZCS+ikqnRSaCen3seG8umeedEIpq1Gvz/M0k5yWT1CKJiqIKtny7lc7ndAIgMjqS2OSYGtdvX7qdlLwUUlomExkdSftR7Vj/1QYAIiIFT7kXb6UXEcHn8bH0v8vodUXPw1qnDfMKSM1LJCXX+VLk8/rwlHvxeXx4Sr0BBZiIqAinThW+fXX69rUf6X9ll1AVHwBPcTl7Fm+kxenOzy4iOpLopDhK1u8kpVsLIuOiiYiMIK13HttnrPQ7X4mKxFfuwVf1b+T1sXHyN7S6aECoquJPqfD5eRyrjpYWVV1yVHUzgKpuFpGq/pmWOC2mKhvdc5Xu4wPP1yIi1+K0vIjMSAu4YAV/fwYQkk8+nqSTjm/0PIB3954a7xWVnkr5mvUAJA46ju3PvE7x7AWkXXA6RV/OIXFIfyJia36IhkLlll1EpiaQ/493KV+7lbgOLWh+zWgi4mJQr4/Vtz1NxZadZIwdSELn2q0pRFj/wCuAkD66H+mj+wOQelJPNv1tMnu+WET2lSPZ+dF80ob3ISI2OuR1qm71J2tpP8ppXRTmFxGXHsv0h2ayc8UumnXN5PjbBhIdv79MxdtKSMzZ3ypOzE5g29JtAHQY054vf/cVKz9YxcAb+vP9m8vpeHoHouIO75/Z8qnr6TLaaSEmZSfQ/4ouPHv6B0TFRtJ6SA5thjSvlUZEmDx+GoLQ64L29LqgAwBdx7Tmo9/O5fv313Hijb1Z+MZKup3Zluj40NapdPMeolPj+eHPUylevY2kTjl0Gj+cxLaZrHl2BpV7SomIjWLn3DUkd8mpoz6w6I43QSD3zN7kntkbgJwRXVn28Ids+eR7OvxiGPnvfEfz07oTGXd4f++qU6xF1ZijOVDVp66vHdrA+donVScAEwBi2+YFNIaVc9f1RKWl4N1bRMFjzxDVPIu4zu3rPd9wSZwiRyTEk33jVQD4ikvYO+VLml13BTte+h++4lJSRp1EbIc2gRTTb+rzUbZqM81/MYaEznlseWYK29+cSfZlw5HICDr8/Zd4i8rY8OhEytYVENem5nhO20evIjojGc/uYtY98Aoxec1I7NGGyMQ4Wv/uUgC8RaXsmDyLVnddRP6T7+EtKiPznMEkdG0VkjpV8VZ6Wf/VBgZc74xxqMfHjuU7Gfyb48numcWcv85j0YtL6P+rvtV+IHXl5Pw7xSTFMOqxUwEo31vOopcXM/JPw5nxx1lUFFbQ89Lutca7QlGn1V/lM/TXzgdz2d4KVn2Zz1Xvn05sUgwf3DmL7z9YR7czav6+XPT8CJKy4inZWcbk66aR3jaFvP5ZxCbHcO4/hu3L6+sXf+DMv5zAp7+fT9neSvpd3pncPs2CXg/1+ihcUUCnX48gpVsLVjzxBetfn0e7q4bS+pKBLLzjTSLjo0nskIVE1v6Q7/v4JcQ2S6JiVwkL7/gfCa0zSOudR1RSLL0fPg+AysIy1r8+nx4PnsXyv35MZWE5rS7sT2qP3KDXpzE2Pb1hR/NPZ6vbnYf7/wL3/Eag+idcHpDvns+r43xQVY0ZRaYkEd+3BxVrNjR4vkpkeirenbv3Pffs2kNkHeNPe97/jJTTR1Ay7ztiWueR+bML2f3WlGBXY5/ozBSiM1P2tZaSh3SjbPXmmmVPiiOxZ1uKvq3dBVM13hSVlkjy8V0oXbGp1jXbJn5FswtPZM/0JcR1aEHur8+m4JXPQ1CbmjbO2kRm1wziM52usITsRBKzE8jumQVA2xFt2LF8R400idkJFG8t3ve8uKCkzvGn755ZSJ+rerP64zU065rJifeewNf//jaEtXGsnbmF7K7pJGbGAbB+7lZSWyaSkB5HZHQEHUfksXnR9lrpqroDEzLi6DC8JVuX7qh1zdwJSxl0dTeWT1lPdrd0Trt/ILOeXBySesRmJROblUxKtxYAZJ3UicIVzp94i9N7MeCpy+n794uJTo4jvmVa7fTueFNMegLNTuzI3h+21Lpm3ctzaHPZIAo+/4Gkzjl0vX0Ua56dEZL6NEQRfOrfcaw6mgPVu8A49/E44J1q5y8RkVgRaQd0Aua53YSFIjJYRAS4slqaoPCVV+ArK9/3uGzZj0S3bF7v+epi2uZRWbADz7adqMdDyfyFxPfpVuOayq3b8e7eS1yX9vgqKpEIccZ3Kj3BrEYNUelJRDVLoXyT8+FWvGgNsa2y8OwpxltU5tapkqKFq4ltWfObta+sAm9p+b7Hxd+tJq51zRZFef4OPDsLSezZ1pmkIQJCSOtUZfXHa/Z1+wEkNIsnMTuRPev2AJA/fzNp7dJqpGnWvRl7NuylcFOh03r5eA2th9Xs8tyzfi8l20tp0a85njKPM+NRBG95zYkZobB8yv5uP4Dk5glsXryDylIPqsqGeVvJaFfzC1BlqYeK4sp9j9fP2Upmh9Qa1+xaX0jRtjLy+mfjKXPGd0TAU+4LST1iMxKJy0qmZIMzNL3r2/UktskAoGJXCQBlW/eybcYKskd0rZHWW1qJp6Ri3+NdX68jsW1mjWtKNu6ifEcRaX1a4SvzuL93gq8i9P9GB6q6j8qf41h1VNRcRP4LnAI0E5GNwP3Ao8AkEbkaWA9cCKCqS0VkErAM8ADj3Rl/ANexf3r6RwR5xp9vbyHb/vWy88TrJeH4vsT37IJn2446zwMUPP4cGeN+QlRaChmXnkPB358F9ZE4dCAxBwSzPW9NIfW8MYAzbrXtXy9S+OkMUs8ZFcxq1NLiF2PZ9Le3UI+XmJx0cm88m8rte8l//B3U5wNVUoZ2J3lgZwDWP/QaLW44C63wsOHRSW69faSc1JOkfh1r5L3tlS/IutyZhp86rCcbHpnIzvfnkVXHVPdg8pR5yJ+7maF3D6lxfvDtx/Pl76bj8/hIzk1i2H1DAfj45k858bcnkJCVwJDbj2fqjZ+iPh+dzupEeoeaM84W/Psb+l/ndCe2H9WOz27/gmWvL6PfL/sSSpWlHtbP3crI3/bfd65Fr0w6jczjtcs+ISJSyOqSTs/znS7nt3/9FafeNxBvuZf3bpsJgM+rdB3TmrZDW9TIe9aTizlhvDMNv8uY1rx360y+/e8KhlwXuokiHX89nGUPf4RWeolrkUrXO0YDsPSB96jcW4pERdD5xpFEJzutx0V3T6bLbaPwVXhYcv+7AKhXyRnZlcxBNWc5rnluJu1+7vzbZo/oypL73mHj5G9p97Oavw+Hh9jGiY2QIN9G1OTEts3Tuu55OpolZRU3ftFRZnDuunAXIahSokrDXYSg+25XHZNtjnJfjvzbAlU9pCmDeT1T9cZJ/s12vbPHlEN+v6PRUdGiMsaYpsxaVA07mseojDHmqKcq+DTCr8MfInKLiCwVkSUi8l8RiRORDBH5RERWuP9Pd68dKiKLRGS+iHR0z6WJyFR3LP+IYIHKGGPCzKsRfh2NEZGWwI3AAHe5uUiclXqqVvLpBHzmPge4DbgAuAdnDB/gd8DDwV5e7lBYoDLGmDByNk4M6soUUUC8iEQBCTi34dS3kk8lzuSyBKBSRDoALVV1GkcQG6MyxpiwCmgr+mYi8nW15xPcBQoAUNVNIvIXnJnQpcDHqvqxiNS3ks8jOIsblAJXAH/BaVEdUSxQGWNMGCkEcjPv9oZm/bljT+cA7YDdwBsicnm97636HTDYTXsSTutLRGQiTmvrNlXd6m/hQsUClTHGhJEiVGpk4xf651RgjapuAxCRycAJuCv5uK2p6iv54F4nwL3AxTg7VdyPs4XSjcBvg1W4g2VjVMYYE2Y+Ivw6/LAeGCwiCW7wGQl8T/0r+VDt3Afunn4JgM89Qrs/jZ+sRWWMMWHk7EcVnJngqjpXRP4HfIOzMs+3OGNQSdSxkg+AiCTgBKqqJW7+BrwJVAA/DUrBDpEFKmOMCbNgLjirqvfjdN1VV47Tuqrr+hJgeLXn04FeQStQEFigMsaYMHJWT7dRmIZYoDLGmDCzJZQaZoHKGGPCKMDp6cckvwOViEQAEarqqXZuNNAT+FxVQ78rnDHGNDnW9deYQFpU/8UZkLsSQER+BfzLfa1SRM5Q1U+DXD5jjGnyAlge6ZgUSBgfDHxY7fntwDNAKjCZI+CmMGOMOdqoQqUv0q/jWBVIoMoGNgG4y8G3A55Q1ULgeY6w6YzGGHM0cGb9+XccqwLp+tsLZLqPT8FZc2qR+9wLxAWxXMYYc8ywrr+GBRKoZgF3iYgHuJma3YAdgY1BLJcxxhwTbNZf4wLp+rsDyMBZMyoOeKDaaxcDs4NXLGOMOXYEc4ffpsjvFpWqrgA6i0imqu444OWbgC1BLdmRQgFv0/q24/M1rfoAFHtiwl2EoMqIKQ53EYJuSLM14S5C0H0ZjEyO8fEnfwR8w28dQQpVXRyc4hhjzLGlaodfU7+AApWIjMNZTbc1tSdPqKp2CFbBjDHmWGEtqoYFsjLF74AHgSXAdzg3/xpjjDkECnh8TWf8SURigPOBMTj33+biNGx2AMuBacBEVV3mb56BtKiuBh5X1VsCSGOMMaYBVfdRHe3cfa1uB24A0nE2bJwHbANKcSbjtQPGA/eKyAzgHlWd2VjegQSqTOC9wIpujDGmMU1kjGoVzqS6+4BJdc1nqCIiQ4HLgakicpuqPtVQxoEEqmlAH+DzANIYY4xpiDaZMarrVPVtfy50W1EzReQBoG1j1wcSqG4GJovIDpybfXfW8ea+APIzxphjXlO54dffIHVAmq3A1sauCyRQ/ej+//n63jPA/IwxxtA0AlVDRCQS6AwI8GP17aL8EUhgeQgnGBljjAmSpjKZoj4i0gt4C2jvnlorIhcEsodhICtTPBBY8YwxxvhDm3Cgwtm38CXgb0Ay8ATwb5yp635pOpP3jTHmKOVD/DqOZCJym7sT/IF6AH9S1SJV3YwTuHoEkndAgUpEWojIX0RkvoisEpF5IvJnEWkeSD7GGGMcquD1Rfh1HOGuAL5zp55XtxS4TUQSRSQbuNY95ze/ay4inXFWpLgRKMK5kasYZ0Ha70SkUyBvbIwxBmg6Gyf2x5ls94GIPCsiGe75XwPX4OxpuBk4HuemX78FEqL/5L5RZ1Udrqo/VdXhODM59rivG2OMCZCq+HUcyVTVq6qPAd2BROBHEblGVb/DiRN93KOTqi4IJO9AAtVw4HequvaAwq3D2ZtqeCBvbIwxZv99VE2gRQWAquar6iU4+xTeJiKzgR6qusQ9KgPNM5BAFQMU1vNaofu6McaYQKgzTuXPcTQQkTgRSVXVz4DeOEvvfSUij4lI8sHkGUig+g749YGzOkREgOvd140xxgSoicz6ay0in+PMYdgpIt8DA1X1YZyA1Q74QUQuDjTvQG/4fR/4XkQm4gyKNQcuBDoBZwT65sYYc6xTmsx9VM+6/x8KlOBMtHtbRHLdIaJzReQM4B8icrWqjvI3Y79bVKo6BScYFQK/BZ4E7sWJnmeq6sf+5mWMMaZKk5n1dzzwR1Wd6+76/hugGftXpEBVP8C5h2p2IBn71aJyN8KaCDymqgPcfUfSgV2qWhLIGxpjjKnpaBl/asT3wFUi8h1QhjMFvQRYX/0iVS0D7g8kY78ClapWiMipwOPu8xK3AMYYYw6BKviO/Jt5/fFL4H9Agft8N/BzNzAdkkDGqGbirM305aG+qTHGmP2Ogm69RqnqdyLSBeiCMwt8uaqWBiPvQALVbTgDY0XA2ziTKWo0WA91PyoReQ44EyhQ1Z7uuQycbse2wFrgIlXd5b52N3A14AVuVNWp7vn+wAtAPM7eWTepqopILM7iiP2BHcDFB94XFgyb7nmEiLhYiBAkIoLm99yEVlay9S//QT0e8PmI79eLtLNqjiV6du5mxwuv491bhIiQeOLxpIw8EW9hEdv/8xK+0lJSzx5NwnE9Adj2rxdIv/Q8otJSg12FGrzFZWx58l3KNxQAQosbziYqM4XN/3gbz64iiBDSTutHxpm115isK218l1Z49hSz6U8T8RaXkXXpCJKP7wrAxkdeJ+eXZxCdcVCzWP2yd91uZt/32b7nRZsK6fmL/nS5uBcAPq+PT37+NvFZCZz0lzG10m+es4Fv/z4b9Srtz+pCtyuPA6BsVykz7/6EisIKel07gLyT2wIw/Y6PGXD7UOKzEkNWp51rC3nvjjn7nu/ZVMzQ63rQ//JOLHh1BYsmrwGF3ue3o//ltReRWTNzC5//+TvUp/Q6rx3H/9z59yjZWc47t86irLCSE8f3oNOIlgC8dfNMTrunH0nZ8SGr07yXV7Jw8loQyOqUypkP9SMqNpJVM7fy6Z8W4fMpx53XhiFXd6mVtr5rSnaW8+YtcygrrOTkG7rTeUQuAP+7aTajf3scySGsT0OaSNcfquoFlgU730AC1WL3/4+7x4GCsR/VCzgr675U7dxdwGeq+qiI3OU+v1NEugOX4AzM5QKfikhn9wf1b5z1pObgBKoxwEc4QW2XqnYUkUtwVtMIeKqkP7Jv/SWRSdU+mKKiyL7lWiLiYlGvl63/9y/Ke3Qhtn2bfZdIZATpPzmTmNZ5+MrK2PLwP4jv1omyH1aQOLg/CQP7UPCPZ0k4ricli5YR07plyIMUwNZnp5DYtyMt77gIrfTiq6hEyyvJHjeKuA4t8JaWs/Y3E0js04HYVlmNpgXYO2MJqcP7kHxiTzY+9CrJx3elcP5y4to3D2mQAkhpk8boFy8AnKD03jmvkXdS232vr5i0hJS2aVQWV9RK6/P6WPCXmZzy+OnEZyfyydVvkzusDant0ln/ySraju1M61PbM+3WKeSd3JZNM9aR3iUzpEEKIKNtMuMmneaWUfnPqPfpOCKXbSv3sGjyGi5/ZQSR0RH8b/wM2g9rTnqb/T9jn1f59JFvufA/w0jOSeCVyz6jw8m5NOuQwg9T1tPjrDZ0HdOK/10/g04jWrJqWj45XdNDGqQKt5by9Wur+MVbpxIdF8lbt89j2ZSN9DyzNR8/vJBLnhpKSk48L1z6BZ1OaUGzDik16lPfNUs/2kivs1vTbUweE6+fRecRuaz4cjM53dLCFqSgacz6E5HzVPWtANO0ANqo6pyGrgukY/Qh4EH3/3Udvw+kgHVR1a+ovXPwOcCL7uMXgXOrnX9dVctVdQ2wEhjkVjxFVWerquIEvXPryOt/wEj3PrCQExGnlQWo14t6vXDAW0emphDTOg+AiLg4optn49m9ByIj0cpK1ONFIgT1ein8bAbJo04Oebm9JeWULltH6ql9nXpERxKZGEdURjJxHVo45Y6PJTYvC8+OvX6lBSco+yo8aKUXIgT1+tj1/lwyzj1wPcvQKvg6n8SWKSS2cD64SwqKyJ+1gfZn1f6WDrBz2TaS81JIaplCZHQkrU/twKbp6wCIiIrAW+7BV+lDIsDn8fHjxCV0vazPYasPwPq5W0nLSyI1N5GdqwvJ7Z1BdHwUEVERtOrfjBWf59e4fsuSnaS3SiItL4nI6Ai6jm7Fqi/z99XJU+7FU7G/TgteXcHAcZ1DXg+fV/GUe/F5fFSWekjKiiN/yU7SWyWSnpdIZHQE3cbk8eOXm2uka+iayGihssyHt8KHiODz+Jj/6ioGjwvfUqWKf8snHQXB7EkRWSgiv6q2zl+dRGSYiEzA+dzu3VjGR8N+VDnu0vCo6mZ39V2Aljgtpiob3XOV7uMDz1el2eDm5RGRPUAmsD2oJRYoePxpECF52PEkDXO6xNTnY8vDj+PZtoOkk08gtl3rerPwbN9JxYZ8Ytu1JrZdK7Y/+1+K5ywg7fzTKZo2m8TB/YiICf1iIJVbdxGZksDmJ96hfO1W4tq3IOfqMUTE7X/vioLdlK3ZTFznPL/TpgzrRf5jk9n75UKyrjiVXVPmk3pKbyJio0Nep+rWf7qKNqd12Pf827/Poc/4QXhK6l7lpXRbMfE5SfueJ2QlsmOZM3bcelRH5tz/OWunrKDP9YNYOXkZbcd2Iiru8G58/cPUjXQd2wqAZh1TmPHEEkp3lxMVG8nqGVto3j29xvWFBaUkN9/fmkjKiWfzYuf7YrexrXn/7rksfX89J93Uk+8mraL7mW2Ijg9tnZJz4jl+XEeeHD2FqLhI2g3Jpv0JOfzwySZSqpU1OTue/MW7aqQtKiir95ruY1vx7t3zWfL+eobf3IMFE9fQ86xWIa9PY5pIz19HnCnpDwH/dG/4XQhsA8pxZoq3BwYAqcBXwGmqOquxjI/mrePr+nqhDZxvKE3NjEWuxek6JDIjLeCC5dx+PVFpqXj3FlHw+NNENc8mrlN7JCKCFvfegq+klG3/eZGKTVuIaVl7hxRfWTnbJrxM+kVnERHvtECyb/i581pxCXunfkmzX17Jjpf/h6+klJTTTqrRhRhM6vVRtnozOdeMJb5zHluf/Ygdk2eQdekIpzylFWz68yRyfj6GyIRYv9NGJsbR6t5LAfAWlbLjrZnk3XExm//1Lr6iMjLOGUJ8l1YhqVMVb6WXTTPW0fu6gQDkz1xHbHocGV2zKPgmv5HU1bi/VTFJMZz0V2dMq2JvOd+/vJChj5zG/Ee+oqKwgi4/7UWzXjnBrkYN3kofq6blM+xGZxwzs30Kg67qwhu/mk5MQhTZndOIiDzgz6COT8mqxn5scjQXPHEiAGV7K5j3/HLO+dsJTH1wAWWFFQy8ojO5fTKDXo/SvRWs+GIz1384mtjkaN66fR5L3l9PZExkvWWtUtd4T9U1ccnRXPTECfveY/ZzK7jgseP58MFvKNtbyaArO5IXgvo0SIPb9SciacAzQE8nd34OLKeOsX53S45/4wSSn6rqSjf9RGCM2yvlXzWc2eAPicgjwPnAaJwJeLlAHM68gB9who4mquoP/uYdyDYfnzdyfNZ4Lgdlq9udV9WfWTX1cSNQ/ZMsD8h3z+fVcb5GGhGJwonqB3Y1oqoTVHWAqg6oMc7kp6pxo8iUJOKP60HFmg01Xo9IiCeucwfKli6vlVa9XrZPeJnEQX1J6Nur1ut7PviUlLEjKJn/HTFtWpJ55YXsfvujgMvor+jMFKIyU4h3W0vJQ7pTtnqLU1aPl03/N4nUk3qRPLhbQGmr2z5pGs1+Moy9MxYT1yGX5jecw7ZXPw9Znapsmb2B9M7NiMtIcMqxaCv5M9bz3vn/ZfZ9n1OwIJ85D3xRI018ViKlW4v2PS/ZVkx8s9q/I0uf/4buP+vL+k9Wkd61GYN+exKLn5of2goBa2ZsIbtrGomZcfvO9TqvHVe+fiqXPHcKcSnRpLWuOQaYnBNP4Zb9k7OKtpaSlFV7vGb2U8sYfE03fvhoPTnd0xjzwACm/3NJSOqxds42UlsmkpARS2R0BF1G5rJx4U6Sc+LYW62shQWlJGXH1UjrzzUAM5/6gaG/6MKyjzbQvFs6ZzzYj2n/CPo8AP+on4d/HgemqGpXnNXKv2f/WH8n4DP3OTiT5C4A7gGuc8/9Dng4kCBVoyqqlao6UVV/rqrdVTVNVeNUtaWqjlTVBwMJUhDYGFUEznfH6kcznOUyOlN3ayUY3gXGuY/HAe9UO3+JiMSKSDucZZzmud2EhSIy2B1/uvKANFV5/QT4/GD/MerjK6/AV1a273HZ9yuIbtkcb2ERvhLnj8dXUUnZDyuIbl5z4oGqsuOlN4hunk3KqSfVyrty6za8e/YS17kDvopKRAQEtNITzCrUEJWeRHSzVMo3Ob2jxYvWENuqGarK5iffJaZlMzLOHhJQ2uoq8nfg2VlEQo+2+Mr318lXEbo6VVn3ySpaV+v2633dIM5+51LOmvxThjw0guz+uQx+oOamABndsijcuJei/L14K72s/3QVLU+s2YVbuGEPpdtLyO7bAm+Zh6phUG+5N+R1+n7KerqOqVme4p3O7+PezSWs+DyfbmNrtlSb90hn1/oidm8qxlvp44epG+hwcosa1+xaV0jRtjJaDciissyLiCACnorQ1CmleTz5i3ZSWepBVVk7t4Bm7ZLJrSrrRqes30/ZSKcDyurPNTvXFVFUUEbrAc2c+rifbqGqT2N8PvHraIyIpAAn4S5npKoVqrqb+sf6K3FmRycAlSLSAWipqtOCWb9DFcgY1Sl1nXcr9jbw8KEWRkT+C5wCNBORjTh3Lz8KTBKRq3HucL7QLc9SEZmEMxXSA4x3Z/yB883gBZx/gI/cA5x/vJdFZCVOS+qSQy3zgXx7C9n2H3fSos9HwsDjiO/RhYqNm9nx4kTw+UCVhP69ie/dHYCCfz5LxhU/wbN9JyVzvyG6ZXM2/+ExANLOGUN8L6e1suedqaSeMxqAxIHHse0/L1L4+UxSz/J7yayDknPNWDb/fTLq8RKdk06LG86h9IcN7J22iNg22ay59T8AZF02kqT+ndjwh1dpfv3ZRGck15m2um2vfb6vGzHlxF5s+tPr7PxgLs0uOSWkdfKUedg6fxMD7hzm1/Vf3TaFgXcNIz4rkX63nsC0Wz5ypqef2YXU9jXHjRc/NZ9ev3S6E1uf1oEZd33Mj28soec1A4Jej+oqSz2sm1PAqHv71zj/7m2zKd1TQWRUBCPvPo64FGd88c3xMxh9f3+SsuMZeddxvHnddHw+pdc5bWnWseZs0ulPLGXYDc7u4V3HtuKdm2fxzWsrGXp995DUpWXvDLqc1pLnLvmCiEghp2sax/2kLRFREZx2dx9ev24m6oPe57Yhq6Mz42/i+Fmcfn9fkrPj672myrQnlnHyDU7Zu49pxZu3zOHrV1cxbHztnoFQC3Ctv2Yi8nW15xNUdUK15+1xxoSeF5E+wAKcNffqG+t/BJgAlOLs0PsXnBbVEUWC0aAQkcuA36hq30Mv0pEltk2eNr/npnAXI6gSsovDXYSg69M8gDGlo0CrhF2NX3SUiZbwtFZC6ZE+by1Q1UP6BhLbvqXmPezfhrerf/rbBt9PRAbgTDIbqqpzReRxnA1vf62qadWu26Wq6QekPQmnpfUfnFnclcBtqro1sBoFX7DW7diG0/1njDEmQEHcj2ojsFFV57rP/wf0o/6xftxzgrPI+O9xerLuB14BbgxG/Q7VIQcqd778rcCqQy+OMcYcg4I0mUJVtwAb3KWMAEbiDI/UN9ZPtXMfuKv+JAA+90g42CoFk99jVCKyhto/qhigar7tBcEqlDHGHDuCfjPvr4FX3V0vVgNX4TRKao31A7i7YYwDqga7/wa8CVQAPw1mwQ5WIPdRTaN2oCoD1gFvqKq1qIwx5mAEce6xqn6Hc1PtgUbWc30JMLza8+lA7XtjAiAiM3HGuiapavmh5AWBzfr72aG+mTHGmAME+YbfI0QlzjT4v4vIizizEwO6d6q6JrEJijHGHNWCe8Nv2Lm3M3XDCVZXAktF5EsRuVhEAl4nLaBAJSJ9RWSyiGwXEY+I9HPPPywitfdDMMYY0zgV/46jiKouV9VbcdZY/RkQCbwGbBSRR0WkfUPpqwtkCaUTcfa57+q+WfW0PuBX/uZljDGmmibWoqrO3eHiZZwbj6cDWcAdwI8i8oaI1F7w9ACBtKgeBabi7P906wGvfYMzV98YY0wglCbZogIQkXgR+bmIzAPm4wSpm3AWqr0OOAF4tbF8Apn11w84390p98DYvt0tgDHGmAA1lR1+q4hIL+CXwGVAIs59W3eqavVVnp8WkS3AG43lF0igKqP+m79aAHsCyMsYY0yVJhaocPahygf+jjPjb3M9163EGVJqUCCBagZws4hUv6O56sd7NRD6fRmMMaYpOgq79RpxIfB2tYXC66Sq31PtHq76BBKofgfMxImU/8MJUuNE5G9Af2BgAHkZY4xx1RpMOfq9i7NZYq0VsEUkEahQ1bq30a6D35MpVHUhzj4nW4Hf4uw/dYP78smqWnsXQGOMMQ3zd8bf0RXMngGerue1p9zDbwFtRa+q3wAjRSQOyAB2u8tvGGOMOShH54y+RgwHbq/ntXeB/wsks4ACVRVVLWP/9u7GGGMOhS/cBQi6bA7YSqSabexfzNwvAQUqERmHs5pua5z+x+pUVTvUTmWMMaZBR1e3nj8KcBa2/aKO13oBOwLJLJBtPn4HPAgsAb4DDnlFXGOMOeZV3fDbtLwP/E5EvlTVRVUn3furfgu8FUhmgbSorgYeV9VbAnkDY4wxDWuCs/7uA04DFojIfJydh1sCg4A1OLsJ+y2QJZQygfcCydwYY4wfmtisP1XdjnPL0iM4M8SPc///R2Cg+7rfAt04sQ92Y68xxphGqOpunJbVfYeaVyCB6mZgsojsAD4EdtZRsCY3d0W8QszOyHAXI6hKPUnhLkLwNbr+8tHF1/TGLPDR9OoULE2w6y+oAglUP7r/f76e1zXA/IwxxkBTnEyBiPTEmdvQhbpniY/0N69AAstDHFW9pMYYcxRQmtx9VCJyPM5w0VqgE7AISMe5tWkjzmK0fvM7UKnqA4FkbIwxxj9NsOvvYWAycAVQCVytqt+IyAjgZeAPgWQW0Fb0xhhjQqCJzfoDegOvsL/UkQCq+jlOkHokkMxsTMkYY8Lt6ApC/ogGilXVJyI7cfYsrLIc6BlIZtaiMsaYMBL1/ziKrMK5wRec8amfi0iEiEQAVwFbAsnMWlTGGBNuTW/W3/vAKcBrOONVHwB7AS+QBNwYSGYWqIwxJtyOrtZSo1T1/mqPPxWRwcAFQAIwRVU/DiQ/C1TGGBNmR1m3XoNEJBo4HVikqmsAVPVb4NuDzdPGqIwxJtya0Kw/d4v5SUDbYOVp+1EZY0w4KUgTu+EXWI2zeWJQ2H5UxhgTbkdJaykAfwZ+KyKfq+q2Q83M9qMyxpgwa0pjVK4RQAawRkTmAJupGY5VVcf5m1kggcr2ozLGGOOPE3GWTtoGdHCP6gIKzbYflTHGhFsTa1Gpartg5mf7URljTDgdfatOHHa2H5UxxoRbEwtUItK6sWtUdb2/+dl+VMYYE25N75N1LY3Xyu+t020/KmOMCSOhSXb9/ZzagSoTOANoD/w+kMysq84YY8IpBDf8ikgk8DWwSVXPFJEMYCLOahFrgYtUdZeIDAX+jXNf7E9VdaWIpLnXjlHVgwqhqvpCPS/9TURexglWfgtoCSUR6Ssik0Vku4h4RKSfe/5hERkTSF7GGGNcwV9C6Sbg+2rP7wI+U9VOwGfuc4DbcBaLvQe4zj33O+Dhgw1SfngFp8XlN78DlYicCMwGuuIs3V49rQ/4VSBvbIwxxhXEQCUieThdbM9UO30O8KL7+EXgXPdxJRCPs6p5pYh0AFqq6rSDrkvjsqm9BF+DAun6exSYilPBSOCGaq99A1zpTyYi8hxwJlCgqj3dc3U2S93X7sZZFcML3KiqU93z/YEXcH7IHwI3qaqKSCzwEtAf2AFcrKpr3TTjgHvdovxBVav+4YJKfT42PPkYUSmp5I67BoC1f/4DEbGxEBGBRETQanztBT52z/yKvfPnAkrKwMGkDT0JAG9REZtffR5faRkZo8aQ1L0XAJtffo6scy4gKiU1FNXYZ+N9D7tlFyQikhZ33gTA9lcmUbpkGZHJSeT+9jd1pt37xXSKZs0FhaShx5MyfJhTp8Iitj39Ir7SUtLOHENCH2fDz4Knnifj4vOJSgtdnfau283s+z7b97xoUyE9f9GfLhc7P1ef18cnP3+b+KwETvpLzY6Ckq1FzP39l5TuKEUioMPZ3eh8cU/KdpUy8+5PqCisoNe1A8g7uS0A0+/4mAG3DyU+KzFk9QHYubaQD+6cve/5nk3FnHBdD/pd1pkFr/zIkrfWgECzjqmMfnAgUbGRjabtOrYN7942k/LCSoaO70nH4c4+eO/cPJOR9/QjKTs+ZPXZsbaQd++Yt+/57k3FnHhddwZe3pGvX13JwslrUVX6nN+OgZd3rDMPn1d58dLPSc6O5yf/PAGAkp3lTL51DuWFlQwb353OI3IBePPm2Yy65ziSQ1inhgQwRtVMRL6u9nyCqk444Jq/A3cAydXO5ajqZgBV3SwiVevwPQJMAEqBK4C/4LSoDomInFTH6RicnX3vBqYHkl8ggaofcL4bDA78sW4HsvzM5wXgCZxgUqWqWfqoiNzlPr9TRLoDlwA9gFzgUxHprKpenH7Va4E5OIFqDPARTlDbpaodReQS4E/AxW4wvB8YgPPdZIGIvFsVEINp96zpxGTl4Csvq3G+5TXXEZmYVGea8i2b2Tt/LnnX34RERpL/wtMkdOlGTLMsChd9S3K/gST3Po78558mqXsvir9fSmxuy5AHqSo5N/2KyKSaH7ZJgweQfPIJ7Hjp9TrTVORvoWjWXJrffiMSGUnBv54hvkdXorOzKF7wHYnHDyCxfx8K/vUMCX16UrJ4GTGtWoY0SAGktElj9IsXAE5Qeu+c18g7qe2+11dMWkJK2zQqiytqpZXICPr8ejAZXZpRWVzBxz9/i5xBLdk6fxNtx3am9antmXbrFPJObsumGetI75IZ8iAFkNE2mSsmjnLrpEwY/R4dh7eksKCUb/+7gnFvjiE6LpL375jN8qkb6HF220bT/jBlPd3PakvX0a2YPH46HYe3ZNW0fLK7pYU0SAFktk3mqkkj95XpX6M+pPOIXLat3MPCyWu58pVTiIyOYNL4mXQY1pyMNrX/rr5+bSWZ7ZKpKPbsO7dsygZ6ntWabmPyeOP6mXQekcvKaZvJ6ZoWtiAFBNKtt11VB9T3oohUNQIWiMgpjb6t6nfAYDftSUC+81Am4rS2blPVrX6Xbr8vqV2rqt0hp7G/m9EvgYxRleE0D+vSAtjjTyaq+hW1bxaur1l6DvC6qpa7+5qsBAaJSAsgRVVnu/2oLx2Qpiqv/wEjRUSA0cAnqrrTDU6f4AS3oPLs2U3JD8tIGXh8QOkqtxUQ17o1ETExSGQk8e06ULxsMQASEYlWVqIeD4igXi+7Z35F2rDhwS5+QOI6ticyob5fCajcspXYtm321Sm2Y3tKFi4BnA98p05ekAjU66Xwi+mknHrKYSq9o+DrfBJbppDYwvnyWVJQRP6sDbQ/q0ud18c3SyCjSzMAohNjSGmTTum2YiKiIvCWe/BV+pAI8Hl8/DhxCV0v63PY6lJl/bytpOUlkZLrBEifV/GUe/F5fFSWeUnMqr/XpXrayKgIPGVevBU+JELweXx889oKBlxZ988mVNbNLSAtL5HU3AR2rC4kt3c60fFRRERF0Kp/M1Z8nl8rzd6tJayevoU+57etcT4yKgJPec06ff3qSo4f1+kw1aYO/nb7+RfMhgJni8ha4HVghIi8Amx1Pzdx/19QPZH7GXkvzmy8+93jFQLcibea4Tjr/VU/hgC5qjpcVWv/ozUgkEA1A7jZnU1SpepHdzWHtrRSjWYp+5eHbwlsqHbdRvdcS/fxgedrpFFVD04AzWwgr6Da9v47ZI49E+SAraVFyH9+AhueeIw982bXSheT05zSNavxlhTjq6igePn3eHbvBiDpuL6UrFhO/gtPkzFyFHvmziK53wAiYmKCXfy6CRQ88TSb//R3CmfM8TtZTG5zylauxlvk1Kl06Q94dznfZxIH9KXs++UUPPkMqaefRuH02SQO6n/46uRa/+kq2py2fxmyb/8+hz7jByERjW8NXry5kN0rtpPZI5vWozqyZe5Gpt36ET2v7s/KyctoO7YTUXGHf2Lt8qkb6DLGud8yOTueAVd24Zmx7/PUae8RmxRN2yHN/UrbdWxr1s3ewuTx0xnyy+58N2kV3c9oQ3T84a3T91M30m1sKwCadUxhw4IdlO4up7LUw+oZW9m7taRWms/+bxGn3NwTOeDvsPvYVqyZtZU3xs9k6K+68c2k1fQ4s/Vhr9OBRP07GqOqd6tqnqq2xemN+lxVLwfeBaoWgR0HvHNA0nHAB+6X+ASceQc+6m+cNFaOaXUcc1V1y8HkF8i/zu+AmcBCnJaKAuNE5G8440EDD6YAjajr00IbOH+waWq+qci1ON2KRKWlN15KV/EPy4hMSiKuZStKVq+s8VreL28gKiUVT1Eh+c89RUxWNvHt9n9AxmTnkH7yCPKfewqJiSW2RS5EOt8JIuPi9411eUtL2PXVF7S47GcUTJ6Et7SUtGEnE9+6rd/lDFTzW8YTlZaKt7CIrU9MILp5NnEdG59dGt08h5TThlPwxNNIbAwxLXMh0vluFBEfT/Z1Vzt1Kilh7ydfkPWLcex47Q18JaWkjDiJ2PahqxOAt9LLphnr6H2d86ubP3MdselxZHTNouCbhr/wVZZUMvOeT+l70xCiE53getJfnQZ6xd5yvn95IUMfOY35j3xFRWEFXX7ai2a9ckJaHwBvpY9V0/I58dfOeFvZ3gpWfbmJq98/g9jkaN6/YzbLPlhH9zPaNJo2Njma8/45bF8+819Yzll/PYGPH/qa8r0V9L+iC7l9MkNen5XTNnPyjT0AaNY+heOv6szEX80gOiGK7M6pRETW/L698qvNJKbH0rx7Ouvn19xhIjY5mgufGLqvTnOf/5Hz/jaYjx78hrLCCgZd0YmWIa5TnUJ/H9WjwCQRuRpYD1xY9YKIJOAEqlHuqb8BbwIVOPsPBszder61qk6q47ULgfWqOtff/PxuUanqQuAkYCvwW5wP/qoJFSer6nJ/86pDfc3SjUCratfl4fShbnQfH3i+RhoRiQJScboa68urFlWdoKoDVHVAZKL/4wul69ZQ/P1S1v75D2x9/RVKV69ky6RXAfaNJUUlJZPYvRdlG2uvHpIy4Hha3XAredeOJzI+gZjMZrWu2fn5J2ScciqFi74ltmUeORdczM6pH/pdxoNRNWYUmZxEQu+elK/1e+UTkk8YRIu7bqb5LdcTkZhAdFbtOu356FNSR4+k+OvviGmVR+ZlF7HrvSlBK399tszeQHrnZsRlOF8aty/aSv6M9bx3/n+Zfd/nFCzIZ84DX9RK5/P4mHXPJ7QZ1YG8U2qvvbn0+W/o/rO+rP9kFeldmzHotyex+Kn5Ia8PwJoZm8npmk5iptO9t37uVlJyE0nIiCUyOoJOI1qyeeEOv9JWN2fCMgZd3Y0fpqwnp1s6ox4YyIwnFoe0LgCrZ2whp2tajTL1Oa8tP3t9JJc9dzJxKdGkt675N7rpux2smLaZf4+dwrt3zWPd/G28d0/tn//Mp35gyDVdWPbRBpp3T+P0B/rz1T+XhrxOdRGff0cgVPVLVT3TfbxDVUeqaif3/zurXVfidsdVus+nq2ovVe2vqj/Wl38jHsGZW1CXbu7rfgvoPipV/UZVR+LMJsnDGScarqrfBpJPHeprlr4LXCIisSLSDugEzHO7BwtFZLDbt3rlAWmq8voJTtNXcWYsjhKRdBFJx/n2MPUQy11Ds9Fn0O6u+2h7x73kXHI58e070vyiy/BVlO+bWOGrKKd05XJiclrUSu8pKgSgcvcuipYuIqlP3xqvV2zfhnfvHuLbd0ArKpzuRQGfx1Mrr2DxlVfgKyvb97jshx+Jya2/6+hA3sIiADw7d1GycDEJA46r8XplwTa8e/YS16lmnbSyMmh1qM+6T1bRulq3X+/rBnH2O5dy1uSfMuShEWT3z2XwAzXHAVWVeQ9PI7ltOl1+2rtWnoUb9lC6vYTsvi3wlnn2dT15y72hrYxr+ZQNdBmz//tYcvMEtizeSWWpB1Vl/bwCMtol+5W2yq51hRRtK6XVgCw8ZV4kQhA5PHVaNmUj3cbk1ThXvNP5fdy7uYQfP8+n+9iaZT75xp6M//h0rvtoDGc/Oog2A7M46+GaHT471xVRtK2U1lV1cn/vPBVhWFc7uGNUR4o+OBPd6jIPqP3H04CD6phV1TLqaY00RkT+C5yCM81yI86gXZ3NUlVdKiKTgGWABxjvzvgDZ9bICzjT0z9yD4BngZdFZCVOS+oSN6+dIvJ7oOqr1UPVv1WEkreoiM2vuGv5+nwk9elHYueuAOS/8DTZ519EVEoqW159EW9JCRIZQdbZ5xMZX7N7eOfHH5ExaiwASX36suWV59kzazoZp4buXmtvYSHbnnbnpnh9JA7oS3x3p+zbnn+V8hWr8BYVs/HeP5B6+iiSTxjE1n89S+alPyEqLZVtz7yEr7gYIiPJuOi8WpMvdr83hbSznPInDujLtgkvUPjlDFLPGEUoeco8bJ2/iQF3DvPr+q9um8LAu4ZRlF/IuikrSe2QwdRxbwLQ65cDyT3BGddZ/NR8ev3S+VBsfVoHZtz1MT++sYSe19Q7UStoKks9rJu7lVPv7b/vXItemXQ6NY9XLv2UiEghu2savS5wum0n3zCdUfcNICk7vs60VWY+uYSh453bB7qOacU7t8zi29dWMOS6+r4wB68+a+cUMObeml/Y3r5tLqV7KoiIiuC0u48jLsXpen1j/EzG3N/Pr9l7Xz2xlJNucMrfbWwek2+ew9evrWTY9d2DX5FGCHWPSxzl4qi/IRQJBDQVVhq6+VhEApkgoW5rq0mJy2uldd3zdDSrTGl6u7EM7newPRRHppbxu8NdhKCLlsPTqjyc/nTc5AUNTRf3R0JOK+146a1+Xbv477ce8vsdDiKyAPhaVX9Zx2tPAcer6nH+5tdYiyqCmg3OLkBznJtytwI5ODfpbgYOZYzKGGOOWU1wUdr/AE+JyF7gafbPsr4WZ5b49YFk1mCgUtVTqh6LyLnA48BgVZ1X7fzxOKtKPB7IGxtjjHE1sUClqk+LSBfgFqB6c1GBx+pYTaNBgYxR/R74XfUg5RZorog8APyB2nPzjTHGNKaJBSoAVf2NiPwbOBXnXtbtwKequjrQvAIJVJ2AbfW8VgDUveCWMcaY+jXhrehVdRWw6lDzCWR6+hqg1sCY65c441bGGGMC1cSmp4vIVW5PW12vPeAuEO63QFpUDwKvisgSnJUpqiZT/ARn64/LAnljY4wxjmBvnHgEuAnnVqG6FAA3s39N1kYFshX96yKyHSdg3Q1E46yuOx8YraqfNZTeGGNM3Zpg119HoL5lPr4HOtTzWp0CuuFXVT/F2WojAmiGs+R80/suYIwxh8tR1q3nJw9OjKiLv1tC7RPQEkpVVNWnqgUWpIwxJgia2BgVzjJJ9e36/iv2rxDklwZbVLYyhTHGhJbQJLv+/ojT+zYXeAbYhHPD7zU4m/CeFkhmtjKFMcaEWxMLVKo6TUR+AvwdeKraS2uBC1T1y0Dys5UpjDEmzKSBNVePVqr6DvCOu0JFJs6choNalNNWpjDGmHA6+safAnKIexUCtjKFMcaEXRMcowJARPrgDBnV2o1TVV/yN59AAlXVyhQf1fGarUxhjDEHqand8CsiacAHwOCqU+7/q4fkkAQqW5nCGGNCoem1qB7GGZc6CZgOnAfsAX4ODMHd0NZftjKFMcaEU9NclHY0Tqyo2o5+o6ouAL50V1S/CbjS38xsZQpjjAm3pheoWgCrVdUrImVAcrXXJgOvB5KZrUxhjDFhVHXDrz/HUWQLkOY+XofT3Vcl4Il3ja1M4QWGqOo8EfHRcNxXVQ2ohWaMMQZoevdRzcAJTu8DLwP3i0hbnDUAxwHvBpJZY4HlIZy97qseN7mfpjHGhNtR1lryx4NArvv4/3AmVlwMJOAEqV8HklljK1M8WO3xA4Fk3GQ0wZvxYpuXhLsIQRcV4Q13EUwjIprgp3FQNMHPmOo7+6pqJXCbexwU66ozxpgwa2r3UQVbQIHK3T74p0Brat9prKoa0GZYxhhjLFA1xu9AJSK/w+l3XAJ8B5SHqEzGGHPsUJriZIqgCqRFdTXwuKreEqrCGGPMsciG7xoWSKDKBN4LVUGMMeaYZYGqQYHc8DsN6BOqghhjzLGoid7wG1SN3fBbPZDdDEwWkR3Ah8DOA6+3lSqMMSZAqjZG1YjGuv481GyUCvB8PdeqH/kZY4w5wLHcWvKHPytT2I/QGGNCyT5lG9TYyhQPHKZyGGPMsUlBvBapGmJddcYYE24WpxpkgcoYY8LMxqgadlD7URljjAmiqpl/jR2NEJFWIvKFiHwvIktF5Cb3fIaIfCIiK9z/p7vnh4rIIhGZLyId3XNpIjJVRCSkdQ6ABSpjjAmzIN5H5QFuU9VuwGBgvIh0B+4CPlPVTsBn7nNwVjS/ALgHuM499zvgYdUjZ868BSpjjAknDeBoLCvVzar6jfu4EPgeaAmcA7zoXvYicK77uBKIx9knqlJEOgAtVXXaoVcseGyMyhhjwshZmcLvxkszEfm62vMJqjqhznydHXX7AnOBHFXdDE4wE5Fs97JHgAlAKXAF8BecFtURxQKVMcaEm/9r+mxX1QGNXSQiScCbwM2qure+4SZV/Q6nixAROQnIdx7KRJzW1m2qutXv0oWIdf0ZY0yYiapfh195iUTjBKlXVXWye3qriLRwX28BFByQRoB7gd8D97vHK8CNQangIbJAZYwx4aQKPj+PRrgB51nge1X9W7WX3gXGuY/HAe8ckHQc8IGq7sIZr/K5R8Ih1i4orOvPGGPCLIj3UQ3FGWtaLCLfuefuAR4FJonI1cB64MJ97y2SgBOoRrmn/obTIqvA2dE97CxQGWNMuAVpJriqzsCZn1GXkfWkKQGGV3s+HegVlAIFiQUqY4wJJwWxDZIadNgDlYi0Al4CmuP0gU5Q1cdFJAOYCLQF1gIXuf2liMjdwNWAF7hRVae65/sDL+DcB/AhcJOqqojEuu/RH9gBXKyqa90043AGDQH+oKpV9xYElfp8bPjXY0SlpJJ75TUArP2/PxARGwsSgURE0Gr8LTXSVGwrYMvrL+97XrlrB5kjx5B8XD82v/o8vtIyMk4bQ1J358vO5pefI+ucC4hKSQ1FFfbxFpey9d/vUr6+ABHIuf5c4ru0YsuTb1O84EciUxNp+9j4OtPuem8Wez77BkSIbZ1NzvhziYiJxrOnmPz/ex1fcRnNfjqCpEHdANj06GvkXHsmURkpIavP3nV7mHHvF/ueF24qpM+1/eh6SQ8AfF4fU656l/isRIb/9bRa6SsKy5nz8Ez2rN4FwOB7h5HVK5uyXaV8dednVBRV0OeX/Wl1chsApt3+KQPvOIGErNB19+9cW8gHd87e93zPpmJOuK4H/S7rzIJXfmTJW2tAoFnHVEY/OJCo2Mga6eu7pmRnOe/eNpPywkqGju9Jx+EtAXjn5pmMvKcfSdnxIavTvJdXsnDyWhDI6pTKmQ/1Iyo2klUzt/Lpnxbh8ynHndeGIVd3qZW2vmtKdpbz5i1zKCus5OQbutN5RC4A/7tpNqN/exzJIaxPg46ce2uPSOFoUVXdOf2NiCQDC0TkE+BnOHdOPyoid+HcOX2ne1f1JUAPIBf4VEQ6q6oX+DdwLTAHJ1CNAT7CCWq7VLWjiFwC/Am42A2G9wMDcG6fWyAi71YFxGDaPWs6MVk5+MrLapxvefV1RCYm1ZkmJiub1r++DXAC3do/PURi954ULvyW5L4DSe59HPkvPE1S914Uf7+U2NyWIQ9SANue+4jE4zqS+5uL0UoPvopKAFKGH0fa2EFs+edbdaar3LGXXR/Npe1jNxARG03+XydROHMJqcP7UjhjMSkn9yHlxF5s/MPLJA3qRtHXy4lr3yKkQQogpU0qp798LuAEpbfOmkieG1QAlk9cRkrbNCqLK+tM//Vjc8kd3JKTHhmBt9KLt8wDwNqPV9P+9E60Oa09n988lVYnt2Hj9PWkd8kMaZACyGibzBUTR7l1UiaMfo+Ow1tSWFDKt/9dwbg3xxAdF8n7d8xm+dQN9Di77b60DV3zw5T1dD+rLV1Ht2Ly+Ol0HN6SVdPyye6WFtIgVbi1lK9fW8Uv3jqV6LhI3rp9HsumbKTnma35+OGFXPLUUFJy4nnh0i/odEoLmnXY/zvj82q91yz9aCO9zm5NtzF5TLx+Fp1H5LLiy83kdEsLX5ACW5S2EYd91t9B3Dl9DvC6qpar6hpgJTDInWKZoqqz3aU+XjogTVVe/wNGurNhRgOfqOpONzh9ghPcgsqzZzcly5eRMuD4g86jdNUKojMyiU7PQCIj0cpK1OMBEdTrZfesr0gbNrzxjA6Rt6SMku/XkTKyHwASHUVkovMHndC9LZFJjfxxe31oRSXq9aLllUSlJzv5REWgFR600oO4ddr1/mzSzxka0vocaOvXm0lqmUxSC+fLQ0lBMZtmbaDj2Z3rvL6yuIKCb7fQwX09MjqSmORYACKiIvCUe/BWepEIwefx8cPEpXS//PB296+ft5W0vCRSchMB54PbU+7F5/FRWeYlMSuuVpr6romMisBT5sVb4dtXp29eW8GAK2u3YoKtRplKPSRlxZG/ZCfprRJJz0skMjqCbmPy+PHLzTXSNXRNZLRQWeZz6iNOfea/uorB4zqFvD4NCeb09KYorGNUft453RKnxVRlo3uu0n184PmqNBvcvDwisgfIrH6+jjTVy3UtTkuNqNT0gOu17YN3yBxzJr7y8gMzJv/5CSBCysDBpA4aUm8ehYu+Jal3XwCS+vRl68RXKfz2azLHnMGeubNI7juAiJiYgMsWqMqtu4hMSWTrk29TvnYLsR1yyb5qLBFxjb93dGYK6WefwOrrHiMiJoqE3h1IPK4jAMkn9mbL4/9j77SFZF1+GrunzCfllOOIiA19napb+8lq2o5qv+/514/Npe8NA/HU05oq3FRIXHocc34/nV0rd5LRpRkDbj2eqPho2o7uwMz7vmTNRyvpO34gP775Pe3HdiQq7vD+mS2fuoEuY1oDkJwdz4Aru/DM2PeJio2kzZDmtB3SvMb1DV3TdWxrPrxnDt+/v45hN/Xiu0mr6H5GG6LjQ1un5Jx4jh/XkSdHTyEqLpJ2Q7Jpf0IOP3yyiZTm+78cJWfHk7+4ZodIUUFZvdd0H9uKd++ez5L31zP85h4smLiGnme1Cnl9GnUMByF/hO0+qgPvnG7o0jrOaQPnDzbN/hOqE1R1gKoOiExMbKBotRX/sIzIxCTiWraq9VretTfQ6oZbaTHuGvbMnUnpmlV15qEeD8U/LCWpVx8AIuPiyR13Da3G30Jsbh7Fy5eR1KM3BW9NYvNrL1K6fm1AZQyI10f56s2kjhpIm79cR0RsNDvfmu5f0qJSiuYvp92TN9N+wm/wlVey96uFTp0S42h5z+W0+fMviW3fguIFP5J8fDe2/Psd8v8ykdLlGxrJ/dB5K71smr6e1iPaAbBxxnri0uPI7Nqs3jTqVXYu30Gn87ty+kvnEhUfxdKXFgEQkxTD8L+NYuwL55DeJZNNMzfQanhb5jw8g6/u/pxtiwvqzTd4dfKxalo+nU/LA6BsbwWrvtzE1e+fwbUfn0VlqYdlH6yrkaaha2KToznvn8O47LVTye6Wzprpm+l0ah4fP/Q17/1mFvkLd4SkHqV7K1jxxWau/3A0v/5kLJWlXpa8v77Oz/MDF11o6Jq45GgueuIErvrvcHK6pbHyqy10PbUlHz74DZNvm8vGENWnQcr+u5YaO45RYQlUAd45vRGo/qmfh7PMx0b38YHna6QRkSggFdjZQF5BU7puDcU/LGXt//2BrRNfoXT1SrZMehVg33hSVFIyid17UbZxfZ15FP/4A7G5eUQlJdd6befnn5BxyqkULvqW2Nw8cs6/mJ0ffxjMKtQQlZlCVGYK8Z2dH3XS4B6Ur9ncSCpHyaLVRGenEZWaiERFknx8tzoD0I43viTjgpPYO2Mxce1zybn+HLa/9mlQ61GX/NkbSe+SSXym8+1726ICNk5fz9vnTmLG775k69f5zLy/5tqcCdkJJGQl0qyn0+BvPaItO5fX/nBb8tx39PxZH9Z9vJqMrs0Ycu+JLPz317WuC7Y1MzaT0zWdxEyn62793K2k5CaSkBFLZHQEnUa0ZPMBH8b+XAMwZ8IyBl3djR+mrCenWzqjHhjIjCcWh6Qea+dsI7Xl/jJ1GZnLxoU7Sc6JY++W0n3XFRaUkpRdsyvTn2sAZj71A0N/0YVlH22gebd0zniwH9P+sSwk9WmIoIjP59dxrDrsgeog7px+F7hERGJFpB3QCZjndhMWishgN88rD0hTlddPgM/dcaypwCgRSXf3YxnlnguaZqPPoN2d99H29nvJufhy4tt3pPlFl+GrKN83scJXUU7pyuXE5LSoM4+iRd+S7Hb7VVexfRvewj3Et+uAVlY4XxMFfB5PMKtQQ1R6MtGZKVRs2g5AyeLVxORl+Ze2WSplP27EV16BqjppW9ZsrVRs3oFnVyEJPdqiFZUQIc6YVWXo6lRl3cc1u/36Xj+A89+7hHPfvogTf38KOQNyGfrgyTXSxGcmkJCTyN51ewDYMj+f1HZpNa7Zu34PJdtKyOnXAk+5B4kAELwV3hDXCJZP2UCXMfu/iyU3T2DL4p1UlnpQVdbPKyCjXc0vQP5cs2tdIUXbSmk1IAtPmTMGJwLe8tDUKaV5PPmL9pdp7dwCmrVLJrdHOrvWF7F7YzHeSh/fT9lIp5Nr/h35c83OdUUUFZTRekAzKsu8zr+RgOcw/BvVKUj7UTVV4eiYDejOaVVdKiKTgGU4MwbHuzP+wNk/5QWc6ekfuQc4gfBlEVmJ05K6xM1rp4j8HpjvXveQqu4MUT1r8BYVsfnV550nPh9JvfuR2LkrAPkvPk32eRcRlZKKr6KCkpU/knXuT2rlsfOTj8g4bSwASb37suXV59kzezoZI4M+H6SGrKtPZ/Pjb6IeL9E56TQffy4Amx97g5Kla/EWlrD62r+SefEppI7sz8Y/vkLz684mvnMeSUO6s+72p5DICGLbNSf1tJrraW5/7TOaXerch5h8Yi/y//Q6uz+YQ+YlI0JaJ0+Zh83z8hl0l3+TN7645WOOv+dEErISGHDbYGbe/yW+Sh9JLZMZfO+wGtcufGoBfX7ZH4C2p7Vn2p2fsXziMnr/ol/Q61FdZamHdXO3cuq9/feda9Erk06n5vHKpZ8SESlkd02j1wVOcJ58w3RG3TegwWuqzHxyCUPH9wSg65hWvHPLLL59bQVDrusRkrq07J1Bl9Na8twlXxARKeR0TeO4n7QlIiqC0+7uw+vXzUR90PvcNmR1dGb8TRw/i9Pv70tydny911SZ9sQyTr6hOwDdx7TizVvm8PWrqxg2vltI6tOgqq4/Uy85gvbGOiLFtWylB97vdLSL6FIU7iIE3cC8dY1fdBTJiS0MdxGCLjYi9K3kw+2RPm8t8Gc184akJuTqkM7X+HXt1IW/P+T3OxrZyhTGGBNu1mBokAUqY4wJq2N7/MkfFqiMMSacFAtUjbBAZYwx4WaTKRpkgcoYY8LsWL5Hyh8WqIwxJpwUv3bvPZZZoDLGmLCyyRSNsUBljDHhZoGqQRaojDEm3CxQNcgClTHGhJONUTXKApUxxoSVgtqsv4ZYoDLGmHCzrr8GWaAyxphwsq6/RlmgMsaYcLMbfhtkgcoYY8LK7qNqjAUqY4wJJ8VaVI2wQGWMMeFmLaoGWaAyxphws0DVIAtUxhgTVmqz/hphgcoYY8JJQe2G3wZZoDLGmHCzFlWDLFA1ojx/4/aVv71t3WF6u2bA9sP0XofLYanTj6F+g/3s3+jocLjq1CYoudgYVYMsUDVCVbMO13uJyNeqOuBwvd/h0NTq1NTqA1ansFMFrzfcpTiiWaAyxpgwU7uPqkEWqIwxJqxsZYrGWKA6skwIdwFCoKnVqanVB6xO4WWL0jZK1CK5McaETWpEpg6OGePXtR+Xv7bgqBl7C6KIcBfAHBtE5AEROWK/FR3p5WuIiJwrIreGuxzm4CigPvXr8IeIjBGR5SKyUkTucs/9SUQWichL1a67QkRuCk2tgssClTGOZ4Ah4S7EQToXsEB1tFJ3h19/jkaISCTwJDAW6A78VET6ACeoam8gUkR6iUg88DPgX6GrWPDYGJVpskQkVlXL/blWVTcCG0NcJL8EUm7TNPjbWvLDIGClqq4GEJHXgbOBGBERIB6oBG4H/qGqlcF645BSVTvsCPkBPOD8utU41wd4F9gFlAIzgWEHXNMReBlY416zGvg3kF5X/kBPYCpQBLxT7Xwn4AP3/DrgPiCikfL5lda99qfAD0AZsBjnw+FL4Et/fi4HltvfugMvuOmrH2sD+RnbEfa/jSnA134eSw54fu0Bef0EeKba8yuAJ4A7gO+AvwItgPfCXe9ADmtRmbAQkX7AdOBb4BdACfAr4FMROUFVF7iX5uK0dG7G+bBtD9wDfEjdXXXvAM8CfwJ8wCnu+beA54HHgLOAB4EN7rnGNJhWRE4DXsUJCLfhrIrwdyAO/xfNOLDc4F/dfw9kAQNxgiNAuVsuf3/GJoxU1b+ZFP6Rut9C/wz8GUBEngHuE5FrgFHAIlX9QxDLEHzhjpR2HBsHB7RYgM+A74GYauci3XNvN5BPFHAiTsuh74H5AzfV9b7AVQecXwx8XF/5Akw7C+ebrlQ7189N+6U/P5cDyx1g3V8ANtZx/UH9jO04eg+cLzBTqz2/G7i72vO+OOOxicBX7rnXgU7hLntDh02mMIedO5B7MvAG4BORKBGJwvk2+ClwUrVrY0TkHhH5QURKcfrXp7svd6kj+7fqedsPDni+BGjtZ5HrTesOXg8A3lT3rx5AVb/B6bLzV61yH0Tdq6f1+2dsmpT5QCcRaSciMcAlOC39Kr/H6bqOxvnSAk4LPuGwljJA1vVnwiED54/kd+5Ri4hEqLP3wSPAr4GHcFouhUAeMBmna+1Am+t5z50HPC+vJ32gaZvh/NEX1JFuq5/5Q93lDrTu1QXyMzZNhKp6ROQGnPHOSOA5VV0Kzm0MwHxVzXefzxaRxThdfwvDVWZ/WKAy4bAb51vck8BLdV1Q7QP0EuAlrdaHLiJJDeR9uO+F2o7T0smu47UcYL2f+dRV7kDrXt1u/P8ZmyZEVT/EGcc88PzbwNvVnv8G+M1hK9ghsEBlDjtVLRaR6Tgz0r5p5AMzAScQVHdVyAoXIFX1isjXwAUi8kBV95+I9Afa4X+gqou/dS/HmXZcvVyB/IyNOaJZoDLhcivwFTBVRJ7F6fpqhjMJIVJV73KvmwKMc7soVgLnAyeEobwNuR/4GHhLRCbg1OMBYAv7Z/AdDH/rvgzIEJHrcKYsl6nqYvz/GRtzRLNAZcJCVb8RkYE4H/L/AFKBbcA3wH+qXfprnAkAf3Sff4hzz9K8w1fahqnqJyJyGU5d3sIJKrfhDFrvOYSs/a37M8Bg4GEgDeder7YB/IyNOaLZorTGhICI5OEErD+q6u/DXR5jjmYWqIw5RO5U8L/hTPvejnNj7h04kyl6qGp9MxGNMX6wrj9jDp0XaI6zVE0mUIxzv9OFFqSMOXTWojLGGHNEs5UpjDHGHNEsUBljjDmiWaAyxhhzRLNAZYwx5ohmgcoYY8wRzQKVMcaYI5oFKmOMMUc0C1TGGGOOaP8P5Ag8PtlsJfEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse # handles arguments\n",
    "import sys; sys.argv=['']; del sys # required to use parser in jupyter notebooks\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch SUSY Example')\n",
    "parser.add_argument('--dataset_size', type=int, default=100000, metavar='DS',\n",
    "                help='size of data set (default: 100000)')\n",
    "parser.add_argument('--high_level_feats', type=bool, default=None, metavar='HLF',\n",
    "                help='toggles high level features (default: None)')\n",
    "parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.05, metavar='LR',\n",
    "                help='learning rate (default: 0.02)')\n",
    "parser.add_argument('--momentum', type=float, default=0.8, metavar='M',\n",
    "                help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=2, metavar='S',\n",
    "                help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# set seed of random number generator\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "grid_search(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With CUDA commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os,sys\n",
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "import torch # pytorch package, allows using GPUs\n",
    "# fix seed\n",
    "seed=17\n",
    "np.random.seed(seed)\n",
    "from torchvision import datasets # load data\n",
    "\n",
    "class SUSY_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"SUSY pytorch dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_file, root_dir, dataset_size, train=True, transform=None, high_level_feats=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            train (bool, optional): If set to `True` load training data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            high_level_festures (bool, optional): If set to `True`, working with high-level features only. \n",
    "                                        If set to `False`, working with low-level features only.\n",
    "                                        Default is `None`: working with all features\n",
    "        \"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        features=['SUSY','lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n",
    "                'missing energy magnitude', 'missing energy phi', 'MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2', \n",
    "                'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n",
    "\n",
    "        low_features=['lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n",
    "                'missing energy magnitude', 'missing energy phi']\n",
    "\n",
    "        high_features=['MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2','S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n",
    "\n",
    "\n",
    "        #Number of datapoints to work with\n",
    "        df = pd.read_csv(root_dir+data_file, header=None,nrows=dataset_size,engine='python')\n",
    "        df.columns=features\n",
    "        Y = df['SUSY']\n",
    "        X = df[[col for col in df.columns if col!=\"SUSY\"]]\n",
    "\n",
    "        # set training and test data size\n",
    "        train_size=int(0.8*dataset_size)\n",
    "        self.train=train\n",
    "\n",
    "        if self.train:\n",
    "            X=X[:train_size]\n",
    "            Y=Y[:train_size]\n",
    "            print(\"Training on {} examples\".format(train_size))\n",
    "        else:\n",
    "            X=X[train_size:]\n",
    "            Y=Y[train_size:]\n",
    "            print(\"Testing on {} examples\".format(dataset_size-train_size))\n",
    "\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # make datasets using only the 8 low-level features and 10 high-level features\n",
    "        if high_level_feats is None:\n",
    "            self.data=(X.values.astype(np.float32),Y.values.astype(int))\n",
    "            print(\"Using both high and low level features\")\n",
    "        elif high_level_feats is True:\n",
    "            self.data=(X[high_features].values.astype(np.float32),Y.values.astype(int))\n",
    "            print(\"Using both high-level features only.\")\n",
    "        elif high_level_feats is False:\n",
    "            self.data=(X[low_features].values.astype(np.float32),Y.values.astype(int))\n",
    "            print(\"Using both low-level features only.\")\n",
    "\n",
    "\n",
    "    # override __len__ and __getitem__ of the Dataset() class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample=(self.data[0][idx,...],self.data[1][idx])\n",
    "\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def load_data(args):\n",
    "\n",
    "    data_file='SUSY.csv'\n",
    "    root_dir=os.path.expanduser('~')+'/ML_review/SUSY_data/'\n",
    "\n",
    "    train_kwargs = {} # CUDA arguments, if enabled\n",
    "    test_kwargs = {}\n",
    "    \n",
    "\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "    \n",
    "        \n",
    "    # load and noralise train and test data\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        SUSY_Dataset(data_file,root_dir,args.dataset_size,train=True,high_level_feats=args.high_level_feats),\n",
    "        batch_size=args.batch_size, shuffle=True, **train_kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        SUSY_Dataset(data_file,root_dir,args.dataset_size,train=False,high_level_feats=args.high_level_feats),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **test_kwargs)\n",
    "    \n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # construct NN\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self,high_level_feats=None):\n",
    "        # inherit attributes and methods of nn.Module\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        if high_level_feats is None:\n",
    "            self.fc1 = nn.Linear(18, 200) # all features\n",
    "        elif high_level_feats:\n",
    "            self.fc1 = nn.Linear(10, 200) # low-level only\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(8, 200) # high-level only\n",
    "\n",
    "\n",
    "        self.batchnorm1=nn.BatchNorm1d(200, eps=1e-05, momentum=0.1)\n",
    "        self.batchnorm2=nn.BatchNorm1d(100, eps=1e-05, momentum=0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(200, 100) # see forward function for dimensions\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Defines the feed-forward function for the NN.\n",
    "\n",
    "        A backward function is automatically defined using `torch.autograd`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : autograd.Tensor\n",
    "            input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        autograd.Tensor\n",
    "            output layer of NN\n",
    "\n",
    "        '''\n",
    "\n",
    "        # apply rectified linear unit\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # apply dropout\n",
    "        #x=self.batchnorm1(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "\n",
    "        # apply rectified linear unit\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # apply dropout\n",
    "        #x=self.batchnorm2(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "\n",
    "        # apply affine operation fc2\n",
    "        x = self.fc3(x)\n",
    "        # soft-max layer\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "import torch.optim as optim # different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc\n",
    "\n",
    "def evaluate_model(args,device,train_loader,test_loader):\n",
    "\n",
    "    # create model\n",
    "    DNN = model(high_level_feats=args.high_level_feats).to(device)\n",
    "    # negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n",
    "    criterion = F.nll_loss\n",
    "    # define SGD optimizer\n",
    "    #optimizer = optim.SGD(DNN.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    #optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    optimizer = optim.Adadelta(DNN.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    def train(epoch):\n",
    "        '''Trains a NN using minibatches.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            Training epoch number.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # set model to training mode (affects Dropout and BatchNorm)\n",
    "        DNN.train()\n",
    "        # loop over training data\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "            label = label.type(torch.LongTensor)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            # zero gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            # compute output of final layer: forward step\n",
    "            output = DNN(data)\n",
    "            # compute loss\n",
    "            loss = criterion(output, label)\n",
    "            # run backprop: backward step\n",
    "            loss.backward()\n",
    "            # update weigths of NN\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print loss at current epoch\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item() ))\n",
    "            \n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    def test():\n",
    "        '''Tests NN performance.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # evaluate model\n",
    "        DNN.eval()\n",
    "\n",
    "        test_loss = 0 # loss function on test data\n",
    "        correct = 0 # number of correct predictions\n",
    "        # loop over test data\n",
    "        for data, label in test_loader:\n",
    "            # compute model prediction softmax probability\n",
    "            label = label.type(torch.LongTensor)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = DNN(data)\n",
    "            # compute test loss\n",
    "            test_loss += criterion(output, label, size_average=False).item() # sum up batch loss\n",
    "            # find most likely prediction\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            # update number of correct predictions\n",
    "            correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        # print test loss\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "\n",
    "        return test_loss, correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    train_loss=np.zeros((args.epochs,))\n",
    "    test_loss=np.zeros_like(train_loss)\n",
    "    test_accuracy=np.zeros_like(train_loss)\n",
    "\n",
    "    epochs=range(1, args.epochs + 1)\n",
    "    for epoch in epochs:\n",
    "\n",
    "        train_loss[epoch-1] = train(epoch)\n",
    "        test_loss[epoch-1], test_accuracy[epoch-1] = test()\n",
    "\n",
    "\n",
    "\n",
    "    return test_loss[-1], test_accuracy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def grid_search(args,device):\n",
    "\n",
    "\n",
    "    # perform grid search over learnign rate and number of hidden neurons\n",
    "    dataset_sizes=[1000, 10000, 100000, 200000] #np.logspace(2,5,4).astype('int')\n",
    "    learning_rates=np.logspace(-5,-1,5)\n",
    "\n",
    "    # pre-alocate data\n",
    "    test_loss=np.zeros((len(dataset_sizes),len(learning_rates)),dtype=np.float64)\n",
    "    test_accuracy=np.zeros_like(test_loss)\n",
    "\n",
    "    # do grid search\n",
    "    for i, dataset_size in enumerate(dataset_sizes):\n",
    "        # upate data set size parameters\n",
    "        args.dataset_size=dataset_size\n",
    "        args.batch_size=int(0.01*dataset_size)\n",
    "\n",
    "        # load data\n",
    "        train_loader, test_loader = load_data(args)\n",
    "\n",
    "        for j, lr in enumerate(learning_rates):\n",
    "            # update learning rate\n",
    "            args.lr=lr\n",
    "\n",
    "            print(\"\\n training DNN with %5d data points and SGD lr=%0.6f. \\n\" %(dataset_size,lr) )\n",
    "\n",
    "            test_loss[i,j],test_accuracy[i,j] = evaluate_model(args,device,train_loader,test_loader)\n",
    "\n",
    "\n",
    "    plot_data(learning_rates,dataset_sizes,test_accuracy)\n",
    "    \n",
    "def plot_data(x,y,data):\n",
    "\n",
    "    # plot results\n",
    "    fontsize=16\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
    "    \n",
    "    cbar=fig.colorbar(cax)\n",
    "    cbar.ax.set_ylabel('accuracy (%)',rotation=90,fontsize=fontsize)\n",
    "    cbar.set_ticks([0,.2,.4,0.6,0.8,1.0])\n",
    "    cbar.set_ticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
    "\n",
    "    # convert axis vaues to to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\mathrm{learning\\\\ rate}$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\mathrm{hidden\\\\ neurons}$',fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 800 examples\n",
      "Using both high and low level features\n",
      "Testing on 200 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.692516\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.815336\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.629408\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.630682\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.741757\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.569121\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.609522\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.683831\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.724576\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.723515\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.654955\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.697068\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.646270\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.726667\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.712167\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.713593\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.760190\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.875029\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.613218\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.724154\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.743630\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.708841\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.701255\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.713606\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.607019\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.699993\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.766468\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.716457\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.689891\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.771521\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.666481\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.761193\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.749452\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.703760\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.670115\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.670829\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.846314\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.766735\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.690590\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.613070\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.705999\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.654028\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.617535\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.777460\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.565717\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.726428\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.734050\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.742769\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.873018\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.682072\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.789377\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.735767\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.808366\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.687613\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.702707\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.751292\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.722536\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.642484\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.749887\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.715161\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.686396\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.663764\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.619073\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.707125\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.709025\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.777194\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.714565\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.721739\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.679030\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.682929\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.794404\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.589068\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.665436\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.680152\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.709832\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.709210\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.730926\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.733414\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.622552\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.692543\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.691447\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.764753\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.731694\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.696342\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.659162\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.748273\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.683819\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.752591\n",
      "\n",
      "Test set: Average loss: 0.6881, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.742588\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.729701\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.659807\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.735594\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.690664\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.678083\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.740712\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.709691\n",
      "\n",
      "Test set: Average loss: 0.6881, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.703835\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.612363\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.669385\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.718624\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.714111\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.655774\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.730234\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.712511\n",
      "\n",
      "Test set: Average loss: 0.6880, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.663591\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.681131\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.689800\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.779433\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.728664\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.625052\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.713355\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.689208\n",
      "\n",
      "Test set: Average loss: 0.6880, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.722294\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.722470\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.755950\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.700394\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.672340\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.627813\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.757718\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.679233\n",
      "\n",
      "Test set: Average loss: 0.6880, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.683537\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.664424\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.706947\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.705331\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.721927\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.688483\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.667736\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.709997\n",
      "\n",
      "Test set: Average loss: 0.6879, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.746985\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.653489\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.673281\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.637144\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.771627\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.705106\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.677221\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.720249\n",
      "\n",
      "Test set: Average loss: 0.6879, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.687580\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.705224\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.714406\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.677350\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.690592\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.653423\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.682522\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.631943\n",
      "\n",
      "Test set: Average loss: 0.6879, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.705781\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.667545\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.606652\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.698576\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.691191\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.663396\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.675020\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.717207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6879, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.682911\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.748574\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.666569\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.629050\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.795412\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.659916\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.720371\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.779990\n",
      "\n",
      "Test set: Average loss: 0.6878, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.731855\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.706652\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.715261\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.677468\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.747170\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.803334\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.750772\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.724585\n",
      "\n",
      "Test set: Average loss: 0.7015, Accuracy: 93/200 (46.500%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.660621\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.668756\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.719361\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.675025\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.755519\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.720347\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.775568\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.707528\n",
      "\n",
      "Test set: Average loss: 0.7011, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.718176\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.765184\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.666382\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.644629\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.670811\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.711543\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.702039\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.624207\n",
      "\n",
      "Test set: Average loss: 0.7007, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.781177\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.768772\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.738284\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.668806\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.813213\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.740446\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.715373\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.730634\n",
      "\n",
      "Test set: Average loss: 0.7004, Accuracy: 93/200 (46.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.698592\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.732623\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.712614\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.710245\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.722984\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.749214\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.760919\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.715670\n",
      "\n",
      "Test set: Average loss: 0.7000, Accuracy: 94/200 (47.000%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.627300\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.608836\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.700076\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.653946\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.766511\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.719798\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.696205\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.702628\n",
      "\n",
      "Test set: Average loss: 0.6996, Accuracy: 95/200 (47.500%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.780129\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.754039\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.661893\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.674271\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.757009\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.730650\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.745434\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.714471\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 95/200 (47.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.741441\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.712593\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.656953\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.729728\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.708667\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.687374\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.740411\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.655357\n",
      "\n",
      "Test set: Average loss: 0.6990, Accuracy: 96/200 (48.000%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.693609\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.780992\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.715065\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.675069\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.639750\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.734832\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.693913\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.694105\n",
      "\n",
      "Test set: Average loss: 0.6986, Accuracy: 95/200 (47.500%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.684028\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.632922\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.650070\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.655508\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.781070\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.660909\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.696755\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.697849\n",
      "\n",
      "Test set: Average loss: 0.6983, Accuracy: 96/200 (48.000%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.665003\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.708547\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.669966\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.612573\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.718053\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.737836\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.673818\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.687008\n",
      "\n",
      "Test set: Average loss: 0.6889, Accuracy: 112/200 (56.000%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.701311\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.659865\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.688284\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.691383\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.685166\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.720256\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.697744\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.715773\n",
      "\n",
      "Test set: Average loss: 0.6868, Accuracy: 116/200 (58.000%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.702550\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.650001\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.619681\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.656018\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.682278\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.694706\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.706203\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.646723\n",
      "\n",
      "Test set: Average loss: 0.6848, Accuracy: 121/200 (60.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.702188\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.660916\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.621012\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.684166\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.647477\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.696529\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.654837\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.679785\n",
      "\n",
      "Test set: Average loss: 0.6831, Accuracy: 121/200 (60.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.678440\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.657469\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.719457\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.632263\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.660207\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.661040\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.641152\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.669679\n",
      "\n",
      "Test set: Average loss: 0.6813, Accuracy: 122/200 (61.000%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.740323\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.704268\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.663290\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.661516\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.731718\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.707994\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.648390\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.697339\n",
      "\n",
      "Test set: Average loss: 0.6797, Accuracy: 125/200 (62.500%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.693119\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.674002\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.653461\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.661569\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.774294\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.737639\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.665006\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.669892\n",
      "\n",
      "Test set: Average loss: 0.6778, Accuracy: 126/200 (63.000%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.690742\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.651763\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.642416\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.684643\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.691742\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.686544\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.720565\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.660856\n",
      "\n",
      "Test set: Average loss: 0.6761, Accuracy: 126/200 (63.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.678028\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.726746\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.667631\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.608688\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.745255\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.654992\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.696138\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.655125\n",
      "\n",
      "Test set: Average loss: 0.6744, Accuracy: 129/200 (64.500%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.755849\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.701237\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.713928\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.653201\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.662792\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.666790\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.676233\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.676210\n",
      "\n",
      "Test set: Average loss: 0.6727, Accuracy: 131/200 (65.500%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.680683\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.644349\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.691516\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.684639\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.661729\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.654996\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.670523\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.657888\n",
      "\n",
      "Test set: Average loss: 0.6715, Accuracy: 138/200 (69.000%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.657559\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.674259\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.683282\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.600661\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.696838\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.660683\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.673586\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.678620\n",
      "\n",
      "Test set: Average loss: 0.6527, Accuracy: 132/200 (66.000%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.662291\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.711160\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.599691\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.662845\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.664608\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.648181\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.686455\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.586869\n",
      "\n",
      "Test set: Average loss: 0.6332, Accuracy: 144/200 (72.000%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.729038\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.642335\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.659342\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.640456\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.628832\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.737597\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.595332\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.696231\n",
      "\n",
      "Test set: Average loss: 0.6124, Accuracy: 145/200 (72.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.727239\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.539546\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.604207\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.628324\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.616742\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.675293\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.471205\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.634450\n",
      "\n",
      "Test set: Average loss: 0.5887, Accuracy: 151/200 (75.500%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.649579\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.581557\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.661936\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.629379\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.612629\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.581284\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.599356\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.680506\n",
      "\n",
      "Test set: Average loss: 0.5703, Accuracy: 146/200 (73.000%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.661524\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.659020\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.534807\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.649761\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.552004\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.634392\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.753788\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.598389\n",
      "\n",
      "Test set: Average loss: 0.5512, Accuracy: 154/200 (77.000%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.615895\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.542306\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.457355\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.792040\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.563413\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.510412\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.529218\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.470923\n",
      "\n",
      "Test set: Average loss: 0.5363, Accuracy: 153/200 (76.500%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.538705\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.416179\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.411662\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.614083\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.571099\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.303893\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.637939\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.462101\n",
      "\n",
      "Test set: Average loss: 0.5252, Accuracy: 153/200 (76.500%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.450352\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.491103\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.496230\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.438708\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.420546\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.679798\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.556988\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.331695\n",
      "\n",
      "Test set: Average loss: 0.5174, Accuracy: 152/200 (76.000%)\n",
      "\n",
      "Training on 8000 examples\n",
      "Using both high and low level features\n",
      "Testing on 2000 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.691783\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.722611\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.714016\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.745090\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.719266\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.709742\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.720149\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.696694\n",
      "\n",
      "Test set: Average loss: 0.6904, Accuracy: 1041/2000 (52.050%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.698635\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.692029\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.685608\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.716584\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.699469\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.700962\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.701519\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.718094\n",
      "\n",
      "Test set: Average loss: 0.6904, Accuracy: 1041/2000 (52.050%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.696943\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.700315\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.710833\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.705290\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.678060\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.709195\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.696463\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.687950\n",
      "\n",
      "Test set: Average loss: 0.6903, Accuracy: 1041/2000 (52.050%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.701566\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.712536\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.718462\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.705868\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.686465\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.682293\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.692192\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.703110\n",
      "\n",
      "Test set: Average loss: 0.6903, Accuracy: 1041/2000 (52.050%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.735252\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.681385\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.693304\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.696177\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.719489\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.728839\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.678851\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.686844\n",
      "\n",
      "Test set: Average loss: 0.6903, Accuracy: 1041/2000 (52.050%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.697850\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.703417\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.744871\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.709047\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.697523\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.696180\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.682200\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.702069\n",
      "\n",
      "Test set: Average loss: 0.6903, Accuracy: 1040/2000 (52.000%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.693387\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.689330\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.717080\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.734888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.716821\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.710860\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.698916\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.726651\n",
      "\n",
      "Test set: Average loss: 0.6903, Accuracy: 1040/2000 (52.000%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.676209\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.689678\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.683373\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.690861\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.714092\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.706744\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.747382\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.712664\n",
      "\n",
      "Test set: Average loss: 0.6903, Accuracy: 1040/2000 (52.000%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.694891\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.690241\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.712923\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.751150\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.683284\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.709367\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.704048\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.695154\n",
      "\n",
      "Test set: Average loss: 0.6903, Accuracy: 1040/2000 (52.000%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.698536\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.722408\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.753581\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.697945\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.699664\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.699128\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.679628\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.732945\n",
      "\n",
      "Test set: Average loss: 0.6903, Accuracy: 1040/2000 (52.000%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.711809\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.714945\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.667338\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.703329\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.695246\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.702063\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.686119\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.700310\n",
      "\n",
      "Test set: Average loss: 0.6902, Accuracy: 949/2000 (47.450%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.731946\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.716374\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.695654\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.721207\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.724605\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.710316\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.717018\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.702081\n",
      "\n",
      "Test set: Average loss: 0.6901, Accuracy: 949/2000 (47.450%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.708422\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.708870\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.724331\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.692741\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.723627\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.691305\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.701948\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.694729\n",
      "\n",
      "Test set: Average loss: 0.6901, Accuracy: 951/2000 (47.550%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.704041\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.702985\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.711268\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.700415\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.688068\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.710982\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.706845\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.731471\n",
      "\n",
      "Test set: Average loss: 0.6900, Accuracy: 951/2000 (47.550%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.688398\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.740509\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.690405\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.713907\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.721073\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.689731\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.732240\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.697392\n",
      "\n",
      "Test set: Average loss: 0.6899, Accuracy: 951/2000 (47.550%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.701545\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.683046\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.717602\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.697643\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.679453\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.717383\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.714708\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.692099\n",
      "\n",
      "Test set: Average loss: 0.6899, Accuracy: 951/2000 (47.550%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.711302\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.705087\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.697716\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.731212\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.714487\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.696952\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.719223\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.693496\n",
      "\n",
      "Test set: Average loss: 0.6898, Accuracy: 951/2000 (47.550%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.709966\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.656195\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.717098\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.695377\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.691510\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.714267\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.696602\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.713037\n",
      "\n",
      "Test set: Average loss: 0.6897, Accuracy: 951/2000 (47.550%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.703493\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.707430\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.688812\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.729519\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.711550\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.708918\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.710836\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.723618\n",
      "\n",
      "Test set: Average loss: 0.6896, Accuracy: 951/2000 (47.550%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.692261\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.704465\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.689802\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.687824\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.708437\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.699588\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.730590\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.716162\n",
      "\n",
      "Test set: Average loss: 0.6896, Accuracy: 951/2000 (47.550%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.736173\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.761263\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.706070\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.706212\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.704098\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.731734\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.677390\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.712621\n",
      "\n",
      "Test set: Average loss: 0.6987, Accuracy: 947/2000 (47.350%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.756798\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.724822\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.704882\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.704457\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.737962\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.690701\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.701100\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.744515\n",
      "\n",
      "Test set: Average loss: 0.6980, Accuracy: 944/2000 (47.200%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.709193\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.686345\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.725156\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.692129\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.704741\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.700448\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.708697\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.695712\n",
      "\n",
      "Test set: Average loss: 0.6974, Accuracy: 942/2000 (47.100%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.718125\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.724204\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.705600\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.718306\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.730452\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.719410\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.711068\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.673831\n",
      "\n",
      "Test set: Average loss: 0.6967, Accuracy: 938/2000 (46.900%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.703943\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.692063\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.712041\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.700148\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.703487\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.675729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.687601\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.718219\n",
      "\n",
      "Test set: Average loss: 0.6961, Accuracy: 938/2000 (46.900%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.698662\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.693972\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.726517\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.702491\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.718215\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.700721\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.702439\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.697848\n",
      "\n",
      "Test set: Average loss: 0.6954, Accuracy: 944/2000 (47.200%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.721935\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.709556\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.713514\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.716939\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.716951\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.702821\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.713171\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.677459\n",
      "\n",
      "Test set: Average loss: 0.6948, Accuracy: 955/2000 (47.750%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.684537\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.681808\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.709564\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.703579\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.708688\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.696805\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.715179\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.708779\n",
      "\n",
      "Test set: Average loss: 0.6942, Accuracy: 960/2000 (48.000%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.722566\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.703471\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.701195\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.719994\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.679721\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.710325\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.677330\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.704075\n",
      "\n",
      "Test set: Average loss: 0.6936, Accuracy: 969/2000 (48.450%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.694670\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.704927\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.719156\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.699180\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.696862\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.687459\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.725591\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.670506\n",
      "\n",
      "Test set: Average loss: 0.6930, Accuracy: 982/2000 (49.100%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.705221\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.685911\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.678751\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.670511\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.713415\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.713451\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.677115\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.679975\n",
      "\n",
      "Test set: Average loss: 0.6809, Accuracy: 994/2000 (49.700%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.691217\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.703847\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.680848\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.715957\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.696911\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.688389\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.676149\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.681442\n",
      "\n",
      "Test set: Average loss: 0.6748, Accuracy: 1159/2000 (57.950%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.686824\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.675065\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.681349\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.689800\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.690723\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.684297\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.671082\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.649301\n",
      "\n",
      "Test set: Average loss: 0.6688, Accuracy: 1295/2000 (64.750%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.682667\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.662543\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.678697\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.687549\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.686786\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.689712\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.676282\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.667325\n",
      "\n",
      "Test set: Average loss: 0.6631, Accuracy: 1384/2000 (69.200%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.663021\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.647993\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.667328\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.659390\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.678546\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.664319\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.680657\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.653716\n",
      "\n",
      "Test set: Average loss: 0.6572, Accuracy: 1411/2000 (70.550%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.638829\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.659975\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.653895\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.663629\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.650675\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.673164\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.691743\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.657554\n",
      "\n",
      "Test set: Average loss: 0.6515, Accuracy: 1445/2000 (72.250%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.668226\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.635158\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.696415\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.676237\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.645522\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.659301\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.659376\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.651087\n",
      "\n",
      "Test set: Average loss: 0.6463, Accuracy: 1468/2000 (73.400%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.649131\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.656337\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.670090\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.656498\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.657531\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.631646\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.657589\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.641968\n",
      "\n",
      "Test set: Average loss: 0.6407, Accuracy: 1476/2000 (73.800%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.660662\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.663723\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.671075\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.637932\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.640665\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.653565\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.626807\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.632890\n",
      "\n",
      "Test set: Average loss: 0.6347, Accuracy: 1481/2000 (74.050%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.626779\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.621660\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.624007\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.600361\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.643157\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.624602\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.628946\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.661149\n",
      "\n",
      "Test set: Average loss: 0.6290, Accuracy: 1489/2000 (74.450%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.692121\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.693525\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.684158\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.667845\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.678463\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.658635\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.648059\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.650226\n",
      "\n",
      "Test set: Average loss: 0.6418, Accuracy: 1437/2000 (71.850%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.645403\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.663667\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.667820\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.638085\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.629177\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.615988\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.577246\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.640793\n",
      "\n",
      "Test set: Average loss: 0.5796, Accuracy: 1512/2000 (75.600%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.594804\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.586328\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.590048\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.650486\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.498594\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.528923\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.528936\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.620643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5304, Accuracy: 1543/2000 (77.150%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.540840\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.552635\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.540234\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.498889\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.494978\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.514219\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.537834\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.510296\n",
      "\n",
      "Test set: Average loss: 0.5018, Accuracy: 1562/2000 (78.100%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.596126\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.498343\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.582616\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.519749\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.564401\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.469663\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.591707\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.607136\n",
      "\n",
      "Test set: Average loss: 0.4866, Accuracy: 1571/2000 (78.550%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.641092\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.602690\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.478311\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.531930\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.530617\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.523417\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.454212\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.553468\n",
      "\n",
      "Test set: Average loss: 0.4771, Accuracy: 1579/2000 (78.950%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.525315\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.510880\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.458599\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.466716\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.441779\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.500913\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.556105\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.608850\n",
      "\n",
      "Test set: Average loss: 0.4715, Accuracy: 1562/2000 (78.100%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.475816\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.520206\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.476579\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.515341\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.581613\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.502908\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.496254\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.495116\n",
      "\n",
      "Test set: Average loss: 0.4688, Accuracy: 1569/2000 (78.450%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.538667\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.504823\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.512271\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.566593\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.560491\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.528297\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.544720\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.529967\n",
      "\n",
      "Test set: Average loss: 0.4640, Accuracy: 1584/2000 (79.200%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.513964\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.454591\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.510875\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.489986\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.445400\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.554381\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.436618\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.541102\n",
      "\n",
      "Test set: Average loss: 0.4616, Accuracy: 1590/2000 (79.500%)\n",
      "\n",
      "Training on 80000 examples\n",
      "Using both high and low level features\n",
      "Testing on 20000 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.708734\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.701809\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.702350\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.706082\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.708414\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.696694\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.700925\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.708750\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 9046/20000 (45.230%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.706776\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.705141\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.701377\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.711879\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.709846\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.713647\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.702579\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.703933\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 9046/20000 (45.230%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.706008\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.705468\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.706265\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.703570\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.704286\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.706696\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.704361\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.705803\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 9047/20000 (45.235%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.705537\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.708121\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.710803\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.707424\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.699389\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.710056\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.709299\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.711394\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 9048/20000 (45.240%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.699637\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.715483\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.712596\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.698547\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.707172\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.700982\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.702432\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.704582\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 9049/20000 (45.245%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.710140\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.702095\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.706128\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.697599\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.698015\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.701429\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.706506\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.702212\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 9050/20000 (45.250%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.706061\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.700985\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.701577\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.704243\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.706924\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.707993\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.704607\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.708714\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 9053/20000 (45.265%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.706574\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.697573\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.704306\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.703492\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.709919\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.698532\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.708319\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.710301\n",
      "\n",
      "Test set: Average loss: 0.6992, Accuracy: 9054/20000 (45.270%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.705437\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.700719\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.698864\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.689139\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.702198\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.703073\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.702627\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.704293\n",
      "\n",
      "Test set: Average loss: 0.6992, Accuracy: 9055/20000 (45.275%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.702163\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.705474\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.703000\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.704662\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.713701\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.700889\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.707059\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.707274\n",
      "\n",
      "Test set: Average loss: 0.6992, Accuracy: 9056/20000 (45.280%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.709153\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.708716\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.706384\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.705240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.704347\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.708498\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.709413\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.709752\n",
      "\n",
      "Test set: Average loss: 0.7006, Accuracy: 9144/20000 (45.720%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.700091\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.706073\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.703010\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.700853\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.709656\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.707867\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.710254\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.705928\n",
      "\n",
      "Test set: Average loss: 0.7004, Accuracy: 9159/20000 (45.795%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.712228\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.703135\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.710251\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.704701\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.709333\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.712427\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.710413\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.708632\n",
      "\n",
      "Test set: Average loss: 0.7002, Accuracy: 9171/20000 (45.855%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.711294\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.702200\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.716263\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.702540\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.706033\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.707823\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.710789\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.705190\n",
      "\n",
      "Test set: Average loss: 0.7001, Accuracy: 9182/20000 (45.910%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.707603\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.711468\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.707517\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.713268\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.707907\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.696206\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.709794\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.713449\n",
      "\n",
      "Test set: Average loss: 0.6999, Accuracy: 9208/20000 (46.040%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.706723\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.709009\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.705767\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.709388\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.705741\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.706174\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.706647\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.712507\n",
      "\n",
      "Test set: Average loss: 0.6997, Accuracy: 9230/20000 (46.150%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.708275\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.699443\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.712204\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.713267\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.710528\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.699357\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.704162\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.711198\n",
      "\n",
      "Test set: Average loss: 0.6996, Accuracy: 9250/20000 (46.250%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.710844\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.707791\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.707525\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.704543\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.706131\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.716707\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.706591\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.712728\n",
      "\n",
      "Test set: Average loss: 0.6994, Accuracy: 9254/20000 (46.270%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.705635\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.706695\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.705836\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.702625\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.702570\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.702222\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.706933\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.707160\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 9267/20000 (46.335%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.711616\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.703931\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.711107\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.708096\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.704721\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.705384\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.702316\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.710471\n",
      "\n",
      "Test set: Average loss: 0.6991, Accuracy: 9279/20000 (46.395%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.695070\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.692913\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.698908\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.696427\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.689864\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.695583\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.694551\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.692358\n",
      "\n",
      "Test set: Average loss: 0.6904, Accuracy: 10653/20000 (53.265%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.700822\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.691112\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.699771\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.698160\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.697827\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.693653\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.690455\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.693573\n",
      "\n",
      "Test set: Average loss: 0.6893, Accuracy: 10872/20000 (54.360%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.695711\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.697447\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.694726\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.688270\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.695452\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.694330\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.690447\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.689261\n",
      "\n",
      "Test set: Average loss: 0.6883, Accuracy: 11054/20000 (55.270%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.695618\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.689234\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.693275\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.689570\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.697913\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.685715\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.689006\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.690777\n",
      "\n",
      "Test set: Average loss: 0.6872, Accuracy: 11227/20000 (56.135%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.687102\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.694253\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.690214\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.692043\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.682697\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.693576\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.694362\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.690348\n",
      "\n",
      "Test set: Average loss: 0.6861, Accuracy: 11402/20000 (57.010%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.686285\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.693147\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.690724\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.692663\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.688519\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.687684\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.688008\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.691094\n",
      "\n",
      "Test set: Average loss: 0.6850, Accuracy: 11550/20000 (57.750%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.693633\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.690583\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.689350\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.692987\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.688562\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.689323\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.699297\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.686092\n",
      "\n",
      "Test set: Average loss: 0.6840, Accuracy: 11710/20000 (58.550%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.689313\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.689545\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.690052\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.686210\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.686596\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.686245\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.695444\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.689464\n",
      "\n",
      "Test set: Average loss: 0.6829, Accuracy: 11880/20000 (59.400%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.680241\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.690185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.686180\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.692684\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.686891\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.693192\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.688867\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.687058\n",
      "\n",
      "Test set: Average loss: 0.6819, Accuracy: 12038/20000 (60.190%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.692274\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.685939\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.690322\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.688277\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.685566\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.687581\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.684482\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.685667\n",
      "\n",
      "Test set: Average loss: 0.6809, Accuracy: 12189/20000 (60.945%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.712830\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.708311\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.712689\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.700895\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.692225\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.695911\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.694191\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.685997\n",
      "\n",
      "Test set: Average loss: 0.6787, Accuracy: 12742/20000 (63.710%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.688872\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.683744\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.690709\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.686040\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.681726\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.687861\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.679501\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.679054\n",
      "\n",
      "Test set: Average loss: 0.6698, Accuracy: 13064/20000 (65.320%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.679919\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.678681\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.680320\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.677337\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.675476\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.682671\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.679176\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.672346\n",
      "\n",
      "Test set: Average loss: 0.6613, Accuracy: 13387/20000 (66.935%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.672511\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.673705\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.668211\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.666578\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.678211\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.665237\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.667918\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.669875\n",
      "\n",
      "Test set: Average loss: 0.6523, Accuracy: 13967/20000 (69.835%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.659946\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.660531\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.653142\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.663932\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.658031\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.657643\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.655291\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.646991\n",
      "\n",
      "Test set: Average loss: 0.6430, Accuracy: 14372/20000 (71.860%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.650271\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.654886\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.656254\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.650348\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.648047\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.651571\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.647472\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.652180\n",
      "\n",
      "Test set: Average loss: 0.6334, Accuracy: 14647/20000 (73.235%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.641161\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.647667\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.643507\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.644388\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.638250\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.645354\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.640993\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.635317\n",
      "\n",
      "Test set: Average loss: 0.6235, Accuracy: 14843/20000 (74.215%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.646264\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.631828\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.639020\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.644447\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.638298\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.636932\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.633550\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.629463\n",
      "\n",
      "Test set: Average loss: 0.6133, Accuracy: 14984/20000 (74.920%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.633993\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.629695\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.627320\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.616495\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.636843\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.628411\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.617675\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.629600\n",
      "\n",
      "Test set: Average loss: 0.6031, Accuracy: 15097/20000 (75.485%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.626912\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.618830\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.614728\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.618872\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.612489\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.623903\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.607748\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.606448\n",
      "\n",
      "Test set: Average loss: 0.5928, Accuracy: 15177/20000 (75.885%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.709090\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.686169\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.692173\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.670609\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.658980\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.650015\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.644947\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.636559\n",
      "\n",
      "Test set: Average loss: 0.6019, Accuracy: 14957/20000 (74.785%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.609541\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.599623\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.605287\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.606261\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.587630\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.573436\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.569318\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.592611\n",
      "\n",
      "Test set: Average loss: 0.5216, Accuracy: 15391/20000 (76.955%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.554373\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.539885\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.562290\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.556636\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.547349\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.551827\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.524449\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.524512\n",
      "\n",
      "Test set: Average loss: 0.4928, Accuracy: 15526/20000 (77.630%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.537987\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.531066\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.531246\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.523827\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.513604\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.492844\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.539917\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.528036\n",
      "\n",
      "Test set: Average loss: 0.4767, Accuracy: 15624/20000 (78.120%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.503175\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.527583\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.512680\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.496249\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.507420\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.497582\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.512379\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.494948\n",
      "\n",
      "Test set: Average loss: 0.4692, Accuracy: 15669/20000 (78.345%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.477490\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.504744\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.505549\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.514994\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.497652\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.519007\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.485382\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.483346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4657, Accuracy: 15711/20000 (78.555%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.505939\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.510497\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.479187\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.506719\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.495853\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.477682\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.496334\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.485189\n",
      "\n",
      "Test set: Average loss: 0.4614, Accuracy: 15742/20000 (78.710%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.486638\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.489707\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.485710\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.492690\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.487344\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.506764\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.505396\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.491135\n",
      "\n",
      "Test set: Average loss: 0.4588, Accuracy: 15768/20000 (78.840%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.492036\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.456755\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.498076\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.495440\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.500192\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.515364\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.485102\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.476062\n",
      "\n",
      "Test set: Average loss: 0.4573, Accuracy: 15784/20000 (78.920%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.483565\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.490242\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.470459\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.481033\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.475190\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.487066\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.489181\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.466201\n",
      "\n",
      "Test set: Average loss: 0.4558, Accuracy: 15804/20000 (79.020%)\n",
      "\n",
      "Training on 160000 examples\n",
      "Using both high and low level features\n",
      "Testing on 40000 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.714920\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.706045\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.703845\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.713874\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.707301\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.708813\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.711608\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.710498\n",
      "\n",
      "Test set: Average loss: 0.7014, Accuracy: 17830/40000 (44.575%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.714298\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.707016\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.715930\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.707023\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.715965\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.710797\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.716917\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.708302\n",
      "\n",
      "Test set: Average loss: 0.7014, Accuracy: 17835/40000 (44.587%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.710144\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.713867\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.709138\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.718175\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.713634\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.714928\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.721153\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.712061\n",
      "\n",
      "Test set: Average loss: 0.7013, Accuracy: 17839/40000 (44.597%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.716995\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.713454\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.710393\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.706030\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.711922\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.712524\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.712443\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.713237\n",
      "\n",
      "Test set: Average loss: 0.7013, Accuracy: 17844/40000 (44.610%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.708556\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.706843\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.709163\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.708136\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.711253\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.710075\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.706859\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.711476\n",
      "\n",
      "Test set: Average loss: 0.7013, Accuracy: 17851/40000 (44.627%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.709035\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.713545\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.715216\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.712008\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.710173\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.712585\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.709105\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.710873\n",
      "\n",
      "Test set: Average loss: 0.7013, Accuracy: 17856/40000 (44.640%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.711079\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.713945\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.711699\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.708984\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.707713\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.717816\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.713349\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.715349\n",
      "\n",
      "Test set: Average loss: 0.7012, Accuracy: 17862/40000 (44.655%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.713626\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.718673\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.707682\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.715146\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.710231\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.712903\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.715795\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.719418\n",
      "\n",
      "Test set: Average loss: 0.7012, Accuracy: 17863/40000 (44.657%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.718346\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.711895\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.709018\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.714371\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.707805\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.709021\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.706576\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.705565\n",
      "\n",
      "Test set: Average loss: 0.7012, Accuracy: 17874/40000 (44.685%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.712652\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.716817\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.709560\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.711739\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.713293\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.714001\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.709512\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.713257\n",
      "\n",
      "Test set: Average loss: 0.7012, Accuracy: 17881/40000 (44.703%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.702009\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.696305\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.700440\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.706026\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.699550\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.705493\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.702176\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.703888\n",
      "\n",
      "Test set: Average loss: 0.6933, Accuracy: 20562/40000 (51.405%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.707278\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.701259\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.701303\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.699611\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.702487\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.701550\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.698934\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.700261\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 20610/40000 (51.525%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.704483\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.701972\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.698005\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.698452\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.700165\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.702218\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.698267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.708231\n",
      "\n",
      "Test set: Average loss: 0.6929, Accuracy: 20646/40000 (51.615%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.701039\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.702912\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.699184\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.701397\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.705772\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.698006\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.698487\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.698631\n",
      "\n",
      "Test set: Average loss: 0.6927, Accuracy: 20693/40000 (51.733%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.697694\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.696492\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.695499\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.701518\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.703389\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.703324\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.699037\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.701619\n",
      "\n",
      "Test set: Average loss: 0.6925, Accuracy: 20753/40000 (51.883%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.696094\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.698937\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.697609\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.698500\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.699097\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.702332\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.697211\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.697479\n",
      "\n",
      "Test set: Average loss: 0.6923, Accuracy: 20820/40000 (52.050%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.698040\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.699029\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.699766\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.698755\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.699170\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.704365\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.698455\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.700012\n",
      "\n",
      "Test set: Average loss: 0.6921, Accuracy: 20902/40000 (52.255%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.703635\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.693896\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.701117\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.704023\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.703976\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.700013\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.697402\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.699988\n",
      "\n",
      "Test set: Average loss: 0.6919, Accuracy: 20984/40000 (52.460%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.704256\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.702870\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.702891\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.702671\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.701754\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.696329\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.701474\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.697274\n",
      "\n",
      "Test set: Average loss: 0.6917, Accuracy: 21056/40000 (52.640%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.702022\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.696084\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.698418\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.699365\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.696350\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.703441\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.698036\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.703773\n",
      "\n",
      "Test set: Average loss: 0.6915, Accuracy: 21198/40000 (52.995%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.714034\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.708412\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.703189\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.707884\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.697797\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.699919\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.698360\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.708948\n",
      "\n",
      "Test set: Average loss: 0.6907, Accuracy: 21786/40000 (54.465%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.694704\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.702186\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.696961\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.704267\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.702717\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.704850\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.694361\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.698531\n",
      "\n",
      "Test set: Average loss: 0.6876, Accuracy: 22074/40000 (55.185%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.703328\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.694654\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.698449\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.698252\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.700837\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.700938\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.688464\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.694478\n",
      "\n",
      "Test set: Average loss: 0.6847, Accuracy: 22949/40000 (57.373%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.700022\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.692605\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.694106\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.692125\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.692293\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.694153\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.691522\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.688950\n",
      "\n",
      "Test set: Average loss: 0.6821, Accuracy: 24396/40000 (60.990%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.690233\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.692002\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.695201\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.690590\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.694123\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.689437\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.691316\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.687279\n",
      "\n",
      "Test set: Average loss: 0.6797, Accuracy: 25844/40000 (64.610%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.685593\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.688768\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.689704\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.686712\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.685892\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.687998\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.691374\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.683554\n",
      "\n",
      "Test set: Average loss: 0.6775, Accuracy: 26838/40000 (67.095%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.687032\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.686778\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.689570\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.690360\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.683590\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.687952\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.692005\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.685764\n",
      "\n",
      "Test set: Average loss: 0.6755, Accuracy: 27203/40000 (68.007%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.686053\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.682555\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.681137\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.683663\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.684996\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.688440\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.680498\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.681538\n",
      "\n",
      "Test set: Average loss: 0.6737, Accuracy: 27179/40000 (67.948%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.681993\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.685358\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.681856\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.688225\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.684307\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.682745\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.684512\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.679925\n",
      "\n",
      "Test set: Average loss: 0.6720, Accuracy: 26957/40000 (67.392%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.683728\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.683114\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.689412\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.683943\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.684096\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.677993\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.679265\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.680989\n",
      "\n",
      "Test set: Average loss: 0.6705, Accuracy: 26740/40000 (66.850%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.709275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.705619\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.705618\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.707649\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.709218\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.707425\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.703910\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.701555\n",
      "\n",
      "Test set: Average loss: 0.6962, Accuracy: 17759/40000 (44.398%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.697859\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.698020\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.697657\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.699592\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.693503\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.698475\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.694010\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.695836\n",
      "\n",
      "Test set: Average loss: 0.6887, Accuracy: 22297/40000 (55.742%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.691859\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.692616\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.689438\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.691286\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.683357\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.689485\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.685749\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.692567\n",
      "\n",
      "Test set: Average loss: 0.6812, Accuracy: 25941/40000 (64.853%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.686124\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.685488\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.682364\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.683159\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.684078\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.684209\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.682088\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.678792\n",
      "\n",
      "Test set: Average loss: 0.6734, Accuracy: 28456/40000 (71.140%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.680559\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.676531\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.680853\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.680745\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.678698\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.673691\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.672888\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.674501\n",
      "\n",
      "Test set: Average loss: 0.6651, Accuracy: 29603/40000 (74.007%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.678902\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.670927\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.674218\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.672204\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.672678\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.669860\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.669607\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.669085\n",
      "\n",
      "Test set: Average loss: 0.6559, Accuracy: 30077/40000 (75.192%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.665990\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.668615\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.663107\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.661319\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.667533\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.660769\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.660641\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.658489\n",
      "\n",
      "Test set: Average loss: 0.6460, Accuracy: 30214/40000 (75.535%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.659146\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.655947\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.656185\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.653904\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.651391\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.652155\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.647925\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.653852\n",
      "\n",
      "Test set: Average loss: 0.6353, Accuracy: 30350/40000 (75.875%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.662117\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.651859\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.648703\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.650055\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.646117\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.652228\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.637109\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.643231\n",
      "\n",
      "Test set: Average loss: 0.6240, Accuracy: 30410/40000 (76.025%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.642152\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.643435\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.642316\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.634043\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.644431\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.637511\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.636310\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.636487\n",
      "\n",
      "Test set: Average loss: 0.6124, Accuracy: 30465/40000 (76.162%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.705480\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.687903\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.673915\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.663067\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.661954\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.645085\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.637154\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.617226\n",
      "\n",
      "Test set: Average loss: 0.5891, Accuracy: 30560/40000 (76.400%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.624491\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.606674\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.603914\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.584144\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.578704\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.554578\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.563436\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.550096\n",
      "\n",
      "Test set: Average loss: 0.5081, Accuracy: 31022/40000 (77.555%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.558828\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.559216\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.548708\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.542478\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.531757\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.530332\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.537933\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.540356\n",
      "\n",
      "Test set: Average loss: 0.4809, Accuracy: 31246/40000 (78.115%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.515789\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.523045\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.522518\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.526433\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.514747\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.521960\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.525075\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.529044\n",
      "\n",
      "Test set: Average loss: 0.4689, Accuracy: 31427/40000 (78.567%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.507509\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.512541\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.503260\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.518698\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.533737\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.517327\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.501582\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.512262\n",
      "\n",
      "Test set: Average loss: 0.4613, Accuracy: 31491/40000 (78.728%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.487381\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.525231\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.495152\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.505267\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.515393\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.494337\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.510116\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.492031\n",
      "\n",
      "Test set: Average loss: 0.4564, Accuracy: 31592/40000 (78.980%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.503549\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.486077\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.516861\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.497782\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.479369\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.500272\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.467202\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.486890\n",
      "\n",
      "Test set: Average loss: 0.4535, Accuracy: 31655/40000 (79.138%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.483559\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.512809\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.477909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.499799\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.479741\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.495204\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.485387\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.478615\n",
      "\n",
      "Test set: Average loss: 0.4512, Accuracy: 31689/40000 (79.222%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.495287\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.462351\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.478350\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.474113\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.474319\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.480780\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.490176\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.480381\n",
      "\n",
      "Test set: Average loss: 0.4495, Accuracy: 31709/40000 (79.272%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.466986\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.477124\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.503551\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.458490\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.474097\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.491769\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.475583\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.498019\n",
      "\n",
      "Test set: Average loss: 0.4480, Accuracy: 31756/40000 (79.390%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-172-81cc7ecb0d73>:60: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(['']+x)\n",
      "<ipython-input-172-81cc7ecb0d73>:61: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(['']+y)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse # handles arguments\n",
    "import sys; sys.argv=['']; del sys # required to use parser in jupyter notebooks\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch SUSY Example')\n",
    "parser.add_argument('--dataset_size', type=int, default=100000, metavar='DS',\n",
    "                help='size of data set (default: 100000)')\n",
    "parser.add_argument('--high_level_feats', type=bool, default=None, metavar='HLF',\n",
    "                help='toggles high level features (default: None)')\n",
    "parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.05, metavar='LR',\n",
    "                help='learning rate (default: 0.02)')\n",
    "parser.add_argument('--momentum', type=float, default=0.8, metavar='M',\n",
    "                help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=2, metavar='S',\n",
    "                help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# set seed of random number generator and put the CUDA commands back into the code\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "grid_search(args,device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* One of the advantages of Pytorch is that it allows to automatically use the CUDA library for fast performance on GPU's. For the sake of clarity, we have omitted this in the above notebook. Go online to check how to put the CUDA commands back into the code above. _Hint:_ study the [Pytorch MNIST tutorial](https://github.com/pytorch/examples/blob/master/mnist/main.py) to see how this works in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have google several times for fixing the error of ‘expected scalar type Long but found Int’. Most of them told me that I need to set my data into longer type tensor, which is an integer using 64 bits. I guess we are having 32 bits in our code. \n",
    "However, I do not know where to add data = data.type(torch.LongTensor). After I have asked Glen, he told me add  label = label.type(torch.LongTensor) inside two for loops. I have also reintalled mkl and\n",
    "\"import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\" (because Tensorflow GPU isn't supported) Then I could finally run the code and obtain the plot.\n",
    "\n",
    "For  lr = $10^{-5}$, the number of hidden neurons' layers has no much effect when(The accuracy barely changed). Increase the number of data points would decrease the accuracy.<br>\n",
    "For lr = 0.0001, the number of data points has no much effect when(The accuracy barely changed). But increase the hidden neuron layers would increase the accuracy. <br>\n",
    "For lr = 0.001, increase the number of data point has no many changes in this section, but more layers of neuron may help.<br>\n",
    "For lr = 0.01, 1000 data points is good enough predict the data. <br>\n",
    "For lr = 0.1, we need to have at least 10000 data points to in order to obtain a good result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After study the [Pytorch MNIST tutorial](https://github.com/pytorch/examples/blob/master/mnist/main.py), I add the following three lines of code and add \"$\\bf{.to(device)}$\" to data, label, and model.  Also changing the optimizer to \"$\\bf{optim.Adadelta(DNN.parameters(), lr=args.lr)}$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For  lr = $10^{-5}$, the number of hidden neurons' layers has no much effect when(The accuracy barely changed). Increase the number of data points would decrease the accuracy.<br>\n",
    "For lr = 0.0001, increase the number of data points increase the accuracy barely. But increase the hidden neuron layers would increase the accuracy. <br>\n",
    "For lr = 0.001, increase the number of data point has no much changes in this section, but more layers of neuron may help.<br>\n",
    "For lr = 0.01, Increase both number of data and number of neuron layers would increase the accuracy of performance. . <br>\n",
    "For lr = 0.1, after 10000 data point there is no much changes, but increase number of neuron layers would increase the accuracy of performance. \n",
    "\n",
    "Overall, the CUDA commands perform worse than the original code (maybe my GPU is bad I don't know). Also, increase the number of data points would increase the accuracy of performance (expect when lr = $10^{-5}$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADR: 2/2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
